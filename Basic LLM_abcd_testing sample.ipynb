{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81c8494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's start\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab0515c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "with open ('input_abcd.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd753d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  244720\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8be63d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd abcd \n"
     ]
    }
   ],
   "source": [
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c1d8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " abcd\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# unique characters\n",
    "chars=sorted(list(set(text)))\n",
    "vocab_size=len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e46abb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1, 0]\n",
      "bda \n"
     ]
    }
   ],
   "source": [
    "# create character to integer mapping\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # input string and output integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # input integers and outpu string\n",
    "\n",
    "print(encode(\"bda \"))\n",
    "print(decode(encode(\"bda \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6359fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question for future 1: large vocab/small sequence how is the data read to split into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05906064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([244720]) torch.int64\n",
      "tensor([1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0])\n"
     ]
    }
   ],
   "source": [
    "#encode entire text and store into torch tensor\n",
    "import torch\n",
    "data=torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # first 100 characters - encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2060d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation set\n",
    "n=int(0.9*len(data))\n",
    "train_data=data[:n]\n",
    "val_data=data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91179280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size=8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98e1daac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([1]) the target: 2\n",
      "when input is tensor([1, 2]) the target: 3\n",
      "when input is tensor([1, 2, 3]) the target: 4\n",
      "when input is tensor([1, 2, 3, 4]) the target: 0\n",
      "when input is tensor([1, 2, 3, 4, 0]) the target: 1\n",
      "when input is tensor([1, 2, 3, 4, 0, 1]) the target: 2\n",
      "when input is tensor([1, 2, 3, 4, 0, 1, 2]) the target: 3\n",
      "when input is tensor([1, 2, 3, 4, 0, 1, 2, 3]) the target: 4\n"
     ]
    }
   ],
   "source": [
    "x=train_data[:block_size]\n",
    "y=train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7401f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove later\n",
    "\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9b4a97ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[3, 4, 0, 1, 2, 3, 4, 0],\n",
      "        [3, 4, 0, 1, 2, 3, 4, 0],\n",
      "        [2, 3, 4, 0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4, 0, 1, 2]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[4, 0, 1, 2, 3, 4, 0, 1],\n",
      "        [4, 0, 1, 2, 3, 4, 0, 1],\n",
      "        [3, 4, 0, 1, 2, 3, 4, 0],\n",
      "        [1, 2, 3, 4, 0, 1, 2, 3]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "#hyperparameters\n",
    "batch_size=4 # independent sequences in parallel\n",
    "block_size=8 # context length\n",
    "max_iters=1000\n",
    "eval_interval=100\n",
    "learning_rate=3e-4\n",
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device='cpu'\n",
    "eval_iters=20\n",
    "n_embd=4\n",
    "n_head=2\n",
    "n_layer=6\n",
    "dropout=0.1\n",
    "\n",
    "#--------------------\n",
    "#torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate a small batch of inputs x and targets y\n",
    "    data=train_data if split=='train' else val_data\n",
    "    ix=torch.randint(len(data)-block_size,(batch_size,))\n",
    "    #print(ix)\n",
    "    x=torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y=x.to(device),y.to(device)\n",
    "    return x,y\n",
    "\n",
    "xb, yb=get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "# for i in range(batch_size):  #batch dimension - rows\n",
    "#     for t in range(block_size): #time dimension - column\n",
    "#         context=xb[i,:t+1]\n",
    "#         target=yb[i,t]\n",
    "#         print(f\"when input is {context} output is: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "62241e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses=torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y=get_batch(split)\n",
    "            X,Y=X.to(device),Y.to(device)\n",
    "            logits,loss=model(X,Y)\n",
    "            losses[k]=loss.item()\n",
    "        out[split]=losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b0f5c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key=nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.query=nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.value=nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k=self.key(x) # (B,T,C head_size)\n",
    "        q=self.query(x) # (B,T,C head_size)\n",
    "        # compute attention scores - affinities\n",
    "        wei = q @ k.transpose(-2,-1) *C**-0.5 # (B,T,16) * (B,16,T) --> (B,T,T)\n",
    "        wei=wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) #(B,T,T)\n",
    "        wei=F.softmax(wei,dim=-1) #(B,T,T)\n",
    "        wei=self.dropout(wei)\n",
    "        # perform weighted aggregation of the values\n",
    "        v=self.value(x) # (B,T,C)\n",
    "        out=wei @ v #(B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd,n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out=torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "715dafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd,4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd,n_embd), #projection layer, added in multi head attention\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd,n_head):\n",
    "        #n_embd - number of embeddings, n_head = number of heads\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa_heads=MultiHeadAttention(n_head,head_size) # heads of dimensional self-attention\n",
    "        self.ffwd=FeedForward(n_embd)\n",
    "        self.ln1=nn.LayerNorm(n_embd)\n",
    "        self.ln2=nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x + self.sa_heads(self.ln1(x)) # apply 4 heads of 8 dimensional self attention (B,T,C)\n",
    "        x=x + self.ffwd(self.ln2(x)) # (B, T, C)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e4faa6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "\n",
    "# class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         #each token directly reads off the logits for the next token from a lookup table\n",
    "#         self.token_embedding_table=nn.Embedding(vocab_size,n_embd)\n",
    "#         self.position_embedding_table=nn.Embedding(block_size,n_embd)\n",
    "#         #self.sa_head=Head(n_embd)\n",
    "#         self.sa_heads=MultiHeadAttention(4,n_embd//4) # 4 heads of 8 dimensional self-attention\n",
    "#         self.ffwd=FeedForward(n_embd)\n",
    "#         self.lm_head=nn.Linear(n_embd,vocab_size)\n",
    "    \n",
    "#     def forward(self,idx,targets=None):\n",
    "#         B,T = idx.shape\n",
    "#         #idx and targets are both B,T tensor of integers\n",
    "#         tok_emb= self.token_embedding_table(idx) #(B,T,C)\n",
    "#         pos_emb=self.position_embedding_table(torch.arange(T,device=device)) #(T,C)\n",
    "#         x = tok_emb + pos_emb #(B,T,C)\n",
    "#         #x=self.sa_head(x) # apply one head of self attention (B,T,C)\n",
    "#         x=self.sa_heads(x) # apply 4 heads of 8 dimensional self attention (B,T,C)\n",
    "#         x=self.ffwd(x) # (B, T, C)\n",
    "#         logits=self.lm_head(x) #(B,T,vocab_size)   \n",
    "\n",
    "#         if targets is None:\n",
    "#             loss = None\n",
    "#         else: \n",
    "#             B,T,C=logits.shape\n",
    "#             logits=logits.view(B*T,C)\n",
    "#             targets=targets.view(B*T)\n",
    "#             loss=F.cross_entropy(logits,targets)\n",
    "\n",
    "#         return logits,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "70b2f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size,n_embd)\n",
    "        #print(self.token_embedding_table.weight)  # abcd 5*4\n",
    "        self.position_embedding_table=nn.Embedding(block_size,n_embd)\n",
    "        #print(self.position_embedding_table.weight)  # abcd 8*4\n",
    "        # self.blocks = nn.Sequential(\n",
    "        #     Block(n_embd,n_head=4),\n",
    "        #     Block(n_embd,n_head=4),\n",
    "        #     Block(n_embd,n_head=4),\n",
    "        #     nn.LayerNorm(n_embd)\n",
    "        # )\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n",
    "        #print(self.blocks)  # abcd \n",
    "        self.ln_f=nn.LayerNorm(n_embd) # final layer norm\n",
    "        #print(self.ln_f.weight)  # abcd  \n",
    "        self.lm_head=nn.Linear(n_embd,vocab_size)\n",
    "        #print(f\"weights - {self.lm_head.weight}\") # abcd 4*5\n",
    "        #print(f\"bias - {self.lm_head.bias}\") # abcd\n",
    "    \n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T = idx.shape\n",
    "        #idx and targets are both B,T tensor of integers\n",
    "        #print(\"Forward\") # abcd\n",
    "        tok_emb= self.token_embedding_table(idx) #(B,T,C)\n",
    "        #print(tok_emb[0,:,:]) # abcd\n",
    "        pos_emb=self.position_embedding_table(torch.arange(T,device=device)) #(T,C)\n",
    "        #print(pos_emb) # abcd\n",
    "        x = tok_emb + pos_emb #(B,T,C)\n",
    "        #print(x[0,:,:]) # abcd\n",
    "        #x=self.sa_head(x) # apply one head of self attention (B,T,C)\n",
    "        x=self.blocks(x) #(B,T,C)\n",
    "        x=self.ln_f(x) #(B,T,C)\n",
    "        logits=self.lm_head(x) #(B,T,vocab_size)\n",
    "        #print(logits) # abcd  \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            B,T,C=logits.shape\n",
    "            logits=logits.view(B*T,C)\n",
    "            targets=targets.view(B*T)\n",
    "            loss=F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #crop idx to last block_size tokens\n",
    "            idx_cond=idx[:,-block_size:]\n",
    "            # get the predictions\n",
    "            logits,loss =self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits=logits[:,-1,:] #becomes B,C\n",
    "            # apply softmax to get the probabilities\n",
    "            probs=F.softmax(logits,dim=-1) # B,C\n",
    "            # sample from the distribution\n",
    "            idx_next=torch.multinomial(probs,num_samples=1) # B,1\n",
    "            # append sample index to the running sequence\n",
    "            idx=torch.cat((idx, idx_next),dim=1) #(B,T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model=BigramLanguageModel()\n",
    "model=model.to(device)\n",
    "#out=m(xb,yb)\n",
    "# logits,loss=model(xb,yb)\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "idx=torch.zeros((1,1),dtype=torch.long)\n",
    "#print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "981a61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "eebd59c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== INITIALIZATION (before forward) =====\n",
      "\n",
      "token_embedding_table.weight\n",
      "shape = (5, 4)\n",
      "tensor([[ 0.6784, -1.2345, -0.0431, -1.6047],\n",
      "        [ 0.3559, -0.6866, -0.4934,  0.2415],\n",
      "        [-1.1109,  0.0915, -2.3169, -0.2168],\n",
      "        [-0.3097, -0.3957,  0.8034, -0.6216],\n",
      "        [-0.5920, -0.0631, -0.8286,  0.3309]])\n",
      "\n",
      "position_embedding_table.weight\n",
      "shape = (8, 4)\n",
      "tensor([[ 0.0349,  0.3211,  1.5736, -0.8455],\n",
      "        [ 1.3123,  0.6872, -1.0892, -0.3553],\n",
      "        [-1.4181,  0.8963,  0.0499,  2.2667],\n",
      "        [ 1.1790, -0.4345, -1.3864, -1.2862],\n",
      "        [-0.8371, -0.9224,  1.8113,  0.1606],\n",
      "        [ 0.3672,  0.1754,  1.3852, -0.4459],\n",
      "        [-1.2024,  0.7078, -1.0759,  0.5357],\n",
      "        [ 1.1754,  0.5612, -0.4527, -0.7718]])\n",
      "\n",
      "blocks.0.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4811, -0.4126, -0.4959, -0.3912],\n",
      "        [-0.3363,  0.2025,  0.1790,  0.4155]])\n",
      "\n",
      "blocks.0.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2582, -0.3409,  0.2653, -0.2021],\n",
      "        [ 0.3035, -0.1187,  0.2860, -0.3885]])\n",
      "\n",
      "blocks.0.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2523,  0.1524,  0.1057, -0.1275],\n",
      "        [ 0.2980,  0.3399, -0.3626, -0.2669]])\n",
      "\n",
      "blocks.0.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4578, -0.1687, -0.1773, -0.4838],\n",
      "        [-0.2863,  0.1249, -0.0660, -0.3629]])\n",
      "\n",
      "blocks.0.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.0117, -0.3415, -0.4242, -0.2753],\n",
      "        [-0.4376, -0.3184,  0.4998,  0.0944]])\n",
      "\n",
      "blocks.0.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1541, -0.4663, -0.3284, -0.1664],\n",
      "        [ 0.0782, -0.4400, -0.2154, -0.2993]])\n",
      "\n",
      "blocks.0.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[ 0.0014, -0.1861, -0.0346, -0.3388],\n",
      "        [-0.3432, -0.2917, -0.1711, -0.3946],\n",
      "        [ 0.4192, -0.0992,  0.4302,  0.1558],\n",
      "        [-0.4234,  0.3460, -0.1376, -0.1917]])\n",
      "\n",
      "blocks.0.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([-0.4150, -0.4971,  0.1431, -0.1092])\n",
      "\n",
      "blocks.0.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[ 0.1947, -0.4103,  0.3712, -0.3670],\n",
      "        [-0.0863,  0.1044,  0.2581,  0.4037],\n",
      "        [ 0.4555, -0.3965,  0.1258, -0.2151],\n",
      "        [-0.0548, -0.3742,  0.4554, -0.3670],\n",
      "        [ 0.2672,  0.1757,  0.1625, -0.2703],\n",
      "        [ 0.4545,  0.1099,  0.0643, -0.4406],\n",
      "        [ 0.2099, -0.0750, -0.2291,  0.4295],\n",
      "        [ 0.1115, -0.2766, -0.2531, -0.0239],\n",
      "        [ 0.2792, -0.1278, -0.2853, -0.1712],\n",
      "        [-0.3735,  0.1783,  0.3870, -0.4707],\n",
      "        [ 0.1161,  0.2583,  0.0907, -0.1781],\n",
      "        [ 0.2610,  0.2628,  0.1870, -0.0879],\n",
      "        [-0.1324,  0.0535, -0.0883, -0.1490],\n",
      "        [ 0.3196,  0.4297, -0.0495, -0.1119],\n",
      "        [ 0.0073, -0.0299,  0.1202,  0.1401],\n",
      "        [-0.4541, -0.1845,  0.4211,  0.1948]])\n",
      "\n",
      "blocks.0.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.0249, -0.3015, -0.3059, -0.4479, -0.1630,  0.1689,  0.3188,  0.2308,\n",
      "        -0.4420, -0.3007, -0.0789,  0.4837,  0.0723, -0.1295,  0.2069, -0.1904])\n",
      "\n",
      "blocks.0.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[-0.1618,  0.1825, -0.1137, -0.0501, -0.2487,  0.1673,  0.1894,  0.0911,\n",
      "         -0.1743, -0.2467, -0.2030,  0.1864,  0.1200,  0.2104,  0.1310,  0.0633],\n",
      "        [-0.0024, -0.1901, -0.2142, -0.2338,  0.1023, -0.1227, -0.0503, -0.1439,\n",
      "         -0.0456, -0.1760, -0.1634,  0.0829, -0.0743,  0.1543, -0.0802, -0.1834],\n",
      "        [-0.0441, -0.1212, -0.0765, -0.2380,  0.1399, -0.1741,  0.1257,  0.1134,\n",
      "          0.1786, -0.1918,  0.1798, -0.1182,  0.0928,  0.2348, -0.0353, -0.0019],\n",
      "        [-0.0576, -0.2087,  0.1200, -0.2482,  0.1552,  0.1871,  0.2364, -0.0590,\n",
      "         -0.2054,  0.0562,  0.1381, -0.2488, -0.0567, -0.1499, -0.0219, -0.1231]])\n",
      "\n",
      "blocks.0.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.1022, -0.0794, -0.2376,  0.2051])\n",
      "\n",
      "blocks.0.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.0.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.0.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.0.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.1.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4192, -0.0784, -0.0569, -0.2041],\n",
      "        [-0.4515, -0.4866,  0.1858, -0.2745]])\n",
      "\n",
      "blocks.1.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3214, -0.0390, -0.1665, -0.1618],\n",
      "        [ 0.0161, -0.1061, -0.1722, -0.2394]])\n",
      "\n",
      "blocks.1.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4069,  0.4193, -0.2001,  0.1325],\n",
      "        [-0.1735,  0.0406,  0.4662,  0.2304]])\n",
      "\n",
      "blocks.1.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4333,  0.1985,  0.4746,  0.1315],\n",
      "        [ 0.3352,  0.4929, -0.0766,  0.1038]])\n",
      "\n",
      "blocks.1.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3475, -0.1030,  0.3703,  0.2563],\n",
      "        [-0.3164, -0.4009, -0.3417, -0.4934]])\n",
      "\n",
      "blocks.1.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3858, -0.1236,  0.3374,  0.0837],\n",
      "        [-0.3803, -0.4011,  0.2487, -0.3719]])\n",
      "\n",
      "blocks.1.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.0616,  0.2399, -0.2314, -0.0545],\n",
      "        [-0.0435, -0.1183, -0.2535, -0.4457],\n",
      "        [-0.4042, -0.2677,  0.4829, -0.2415],\n",
      "        [-0.3358,  0.1212,  0.1378,  0.2740]])\n",
      "\n",
      "blocks.1.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.3801,  0.2784, -0.4958,  0.0443])\n",
      "\n",
      "blocks.1.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[ 0.3029, -0.0462, -0.2946,  0.4767],\n",
      "        [-0.1870, -0.2847, -0.4508,  0.0223],\n",
      "        [ 0.2216,  0.1107,  0.0989, -0.3792],\n",
      "        [-0.4669,  0.0088,  0.4559,  0.2885],\n",
      "        [-0.2911, -0.0649, -0.3686, -0.2412],\n",
      "        [ 0.0905,  0.2723,  0.4142, -0.4591],\n",
      "        [ 0.3343, -0.3526,  0.1872,  0.4231],\n",
      "        [ 0.0070,  0.4549, -0.4260, -0.1910],\n",
      "        [ 0.2916, -0.1089, -0.1024, -0.2084],\n",
      "        [ 0.3447,  0.2453,  0.1602, -0.2810],\n",
      "        [-0.4059,  0.0541,  0.1481, -0.2309],\n",
      "        [-0.1399,  0.3377,  0.0398,  0.0226],\n",
      "        [-0.1231, -0.4528, -0.4701, -0.2390],\n",
      "        [-0.2542,  0.1558, -0.1456, -0.1956],\n",
      "        [ 0.4767,  0.1742,  0.3565, -0.2421],\n",
      "        [-0.2042,  0.1838, -0.3331, -0.3269]])\n",
      "\n",
      "blocks.1.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.0241, -0.1829, -0.3748,  0.2966,  0.4021,  0.0811, -0.0871, -0.4631,\n",
      "        -0.1821,  0.1273,  0.2358, -0.0632, -0.1977,  0.2786, -0.3982,  0.3160])\n",
      "\n",
      "blocks.1.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[-0.0970,  0.0038, -0.0494,  0.0303, -0.0755,  0.1818, -0.0065,  0.1951,\n",
      "          0.2404, -0.1218, -0.1824,  0.2006,  0.1959, -0.1909, -0.0193, -0.2465],\n",
      "        [-0.2046,  0.0483,  0.0665,  0.0530, -0.0680,  0.2306,  0.0357, -0.1475,\n",
      "         -0.0142,  0.0600,  0.0875, -0.1768,  0.0937, -0.1277, -0.2077, -0.1366],\n",
      "        [ 0.2411,  0.2137,  0.2239,  0.1468,  0.1889, -0.0335, -0.1376,  0.1249,\n",
      "         -0.1295, -0.1687, -0.0798,  0.0525,  0.1287, -0.0971, -0.1471,  0.0337],\n",
      "        [-0.1474, -0.1628,  0.1303, -0.0420,  0.2284,  0.2432,  0.0748,  0.0860,\n",
      "          0.0576,  0.0039, -0.0182,  0.0034,  0.0934,  0.2324, -0.0648, -0.1057]])\n",
      "\n",
      "blocks.1.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0605, -0.1208,  0.0425,  0.1866])\n",
      "\n",
      "blocks.1.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.1.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.1.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.1.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.2.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3910,  0.2296, -0.3680, -0.2684],\n",
      "        [-0.1099, -0.0922,  0.0411, -0.4590]])\n",
      "\n",
      "blocks.2.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1556, -0.3814, -0.3164, -0.4157],\n",
      "        [ 0.4357, -0.4735,  0.3772, -0.0168]])\n",
      "\n",
      "blocks.2.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.0581,  0.3127, -0.0462,  0.3136],\n",
      "        [ 0.3615, -0.4341,  0.1924,  0.0944]])\n",
      "\n",
      "blocks.2.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1075,  0.0730,  0.1368, -0.2405],\n",
      "        [-0.0640,  0.4751,  0.3359, -0.0188]])\n",
      "\n",
      "blocks.2.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4703,  0.0219, -0.3405,  0.4066],\n",
      "        [-0.3035, -0.0361, -0.1110,  0.0890]])\n",
      "\n",
      "blocks.2.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4705,  0.0475,  0.2896,  0.3881],\n",
      "        [ 0.4037, -0.1727, -0.1118,  0.2410]])\n",
      "\n",
      "blocks.2.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.1364,  0.2341, -0.1092, -0.3391],\n",
      "        [ 0.2035,  0.0767,  0.2229,  0.4967],\n",
      "        [ 0.3414,  0.4740,  0.0268, -0.4301],\n",
      "        [-0.3508, -0.3106, -0.4406, -0.2506]])\n",
      "\n",
      "blocks.2.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([-0.4603, -0.4613, -0.2988, -0.4929])\n",
      "\n",
      "blocks.2.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.3069,  0.1907,  0.4170, -0.1487],\n",
      "        [-0.1454,  0.2670, -0.2467, -0.2364],\n",
      "        [ 0.3081, -0.4357,  0.0611,  0.4417],\n",
      "        [ 0.0857,  0.1360, -0.2912, -0.0069],\n",
      "        [ 0.0275,  0.1227,  0.1943,  0.4345],\n",
      "        [-0.3816,  0.0150, -0.2498, -0.3955],\n",
      "        [-0.0400, -0.4401,  0.3489,  0.0579],\n",
      "        [-0.2695,  0.2613, -0.4732, -0.1934],\n",
      "        [-0.0974, -0.4249, -0.3179, -0.0816],\n",
      "        [ 0.3794,  0.4828,  0.3181, -0.2986],\n",
      "        [-0.3271,  0.4363,  0.1769,  0.0133],\n",
      "        [ 0.0677, -0.4018, -0.1669,  0.4813],\n",
      "        [-0.1233, -0.0251, -0.4152, -0.2797],\n",
      "        [-0.0102, -0.3106, -0.0620,  0.2035],\n",
      "        [-0.4891,  0.1485, -0.3306, -0.2440],\n",
      "        [ 0.1920,  0.3976, -0.1367, -0.2053]])\n",
      "\n",
      "blocks.2.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.4521, -0.2578, -0.4378, -0.1144,  0.1020, -0.4684,  0.4366,  0.3137,\n",
      "        -0.4895, -0.2388,  0.1631, -0.1027, -0.0545, -0.2258,  0.4016, -0.2795])\n",
      "\n",
      "blocks.2.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[ 0.2073,  0.0161,  0.0503,  0.1950, -0.0412, -0.1423, -0.0404,  0.2028,\n",
      "         -0.1855,  0.0567, -0.2457,  0.1311,  0.0924,  0.0106,  0.1073,  0.0003],\n",
      "        [ 0.1383, -0.1979, -0.0367,  0.1109,  0.2490,  0.1273, -0.1818,  0.1923,\n",
      "         -0.0557, -0.0534, -0.2272, -0.0394,  0.1768,  0.0349, -0.1456,  0.0770],\n",
      "        [-0.0802,  0.2282, -0.2170, -0.0790, -0.2414, -0.0985,  0.0788,  0.2407,\n",
      "          0.0420,  0.2451,  0.0489,  0.1444,  0.2004,  0.2090, -0.1399,  0.2298],\n",
      "        [ 0.1514, -0.1169, -0.1193, -0.2097,  0.0628, -0.2026,  0.1056,  0.0789,\n",
      "         -0.2172,  0.0681, -0.0203,  0.1142,  0.1434, -0.2485,  0.2293,  0.2097]])\n",
      "\n",
      "blocks.2.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0995, -0.2285, -0.0893, -0.0725])\n",
      "\n",
      "blocks.2.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.2.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.2.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.2.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.3.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1285,  0.2820,  0.1818,  0.3961],\n",
      "        [-0.1873,  0.1683,  0.1779, -0.4163]])\n",
      "\n",
      "blocks.3.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4850, -0.2594,  0.3423, -0.4707],\n",
      "        [-0.4352,  0.2801,  0.2698,  0.4112]])\n",
      "\n",
      "blocks.3.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3775, -0.3659,  0.2565,  0.4348],\n",
      "        [ 0.2992,  0.0783,  0.1648,  0.4746]])\n",
      "\n",
      "blocks.3.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3226, -0.2270,  0.3497, -0.3421],\n",
      "        [-0.2757,  0.3650,  0.1578,  0.1615]])\n",
      "\n",
      "blocks.3.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2119, -0.0069,  0.4576, -0.3001],\n",
      "        [ 0.0039,  0.2378, -0.3452,  0.4856]])\n",
      "\n",
      "blocks.3.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2498, -0.1201, -0.1353, -0.3258],\n",
      "        [-0.4906,  0.2819,  0.1328, -0.4683]])\n",
      "\n",
      "blocks.3.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.3218,  0.4942,  0.1911,  0.2006],\n",
      "        [-0.2991, -0.2192, -0.0755, -0.0914],\n",
      "        [-0.3426,  0.0412,  0.0497, -0.0633],\n",
      "        [ 0.0693, -0.1982,  0.1301,  0.1886]])\n",
      "\n",
      "blocks.3.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([-0.2634, -0.4958,  0.2617,  0.1193])\n",
      "\n",
      "blocks.3.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.2543,  0.4819, -0.2261,  0.3379],\n",
      "        [ 0.2537, -0.4192,  0.3225, -0.4597],\n",
      "        [-0.2770, -0.0834, -0.3370,  0.4885],\n",
      "        [-0.1003,  0.1986, -0.4465,  0.2878],\n",
      "        [-0.1554, -0.3803,  0.0731,  0.2422],\n",
      "        [ 0.4327, -0.3054, -0.2461,  0.0961],\n",
      "        [ 0.1356,  0.1922,  0.2744, -0.1134],\n",
      "        [ 0.2778,  0.3686, -0.1306,  0.3557],\n",
      "        [ 0.2443,  0.4410, -0.2841, -0.2469],\n",
      "        [-0.1446,  0.0254,  0.3001, -0.2854],\n",
      "        [ 0.2503, -0.1792,  0.3021, -0.0237],\n",
      "        [-0.4380, -0.2751, -0.3619,  0.2480],\n",
      "        [-0.3353, -0.0417,  0.1079, -0.2742],\n",
      "        [ 0.1442, -0.4882, -0.3578, -0.4531],\n",
      "        [-0.1512, -0.1821,  0.0716, -0.0925],\n",
      "        [ 0.2350,  0.4584,  0.1794, -0.1970]])\n",
      "\n",
      "blocks.3.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.4682,  0.1811, -0.2477,  0.2544,  0.3342,  0.1929,  0.4692,  0.4749,\n",
      "         0.1059, -0.3643,  0.4467, -0.2372, -0.2362,  0.4184,  0.3874,  0.1511])\n",
      "\n",
      "blocks.3.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[ 0.0157, -0.2103, -0.0260,  0.2398,  0.0637,  0.0214, -0.0519, -0.0872,\n",
      "          0.1490,  0.0154,  0.1626, -0.0442,  0.1092,  0.1032,  0.0399,  0.1571],\n",
      "        [ 0.1567,  0.2317,  0.1922, -0.0639, -0.2117,  0.0457, -0.0022, -0.0652,\n",
      "         -0.0419,  0.0118,  0.1824,  0.0779, -0.0888, -0.1028, -0.0619, -0.0966],\n",
      "        [ 0.2248,  0.1324,  0.2257,  0.0008,  0.0504,  0.0867, -0.2366,  0.0223,\n",
      "         -0.0167, -0.1402, -0.1940,  0.2213,  0.2033,  0.1159,  0.2386, -0.1015],\n",
      "        [-0.0432,  0.0947, -0.0413, -0.0491, -0.2066,  0.0672, -0.1511,  0.0091,\n",
      "          0.2437, -0.0770, -0.0788,  0.1508, -0.0919, -0.0215,  0.2334, -0.1025]])\n",
      "\n",
      "blocks.3.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.1789, -0.1399, -0.0693, -0.1186])\n",
      "\n",
      "blocks.3.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.3.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.3.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.3.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.4.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2595,  0.2020,  0.0850, -0.1600],\n",
      "        [-0.3885, -0.1574, -0.2110, -0.1627]])\n",
      "\n",
      "blocks.4.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4511,  0.1077, -0.3674, -0.3894],\n",
      "        [-0.4085,  0.2087, -0.3010, -0.2064]])\n",
      "\n",
      "blocks.4.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3919,  0.2655,  0.2867, -0.4748],\n",
      "        [-0.3585, -0.1888,  0.4130,  0.0512]])\n",
      "\n",
      "blocks.4.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3739,  0.0031, -0.3883, -0.1095],\n",
      "        [-0.1375,  0.4328,  0.1549, -0.0872]])\n",
      "\n",
      "blocks.4.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.0845, -0.1443,  0.1965,  0.1978],\n",
      "        [ 0.1343, -0.1949,  0.4266, -0.0722]])\n",
      "\n",
      "blocks.4.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1947,  0.3132,  0.4075,  0.4976],\n",
      "        [ 0.1481, -0.1704,  0.2539,  0.4290]])\n",
      "\n",
      "blocks.4.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.4904, -0.0619, -0.3410,  0.0932],\n",
      "        [ 0.2068, -0.1033, -0.0418,  0.2251],\n",
      "        [-0.0840, -0.4199,  0.4001, -0.2517],\n",
      "        [-0.0549,  0.0472, -0.0300, -0.4703]])\n",
      "\n",
      "blocks.4.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.2294, -0.2271, -0.2593,  0.1195])\n",
      "\n",
      "blocks.4.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-2.6093e-01, -2.3108e-01, -1.6847e-01, -1.8784e-01],\n",
      "        [-2.0882e-01, -1.3484e-01,  1.2994e-01, -4.0461e-01],\n",
      "        [-3.0264e-01,  7.2957e-03,  6.9534e-02,  2.7615e-01],\n",
      "        [-3.5122e-01,  1.5960e-01,  2.8419e-01,  2.7763e-01],\n",
      "        [-4.6571e-01, -1.9080e-01, -4.2978e-01, -3.1641e-01],\n",
      "        [ 2.7849e-01, -7.4660e-02,  2.1236e-01, -2.9350e-01],\n",
      "        [ 7.5980e-02, -3.0243e-01,  2.4995e-01, -2.1869e-01],\n",
      "        [-1.2538e-01, -4.3382e-01,  1.6518e-03,  4.7474e-01],\n",
      "        [ 2.4269e-01, -2.6677e-01,  6.7244e-03, -5.4824e-02],\n",
      "        [-4.0254e-01,  3.9205e-01,  8.0603e-03,  1.0530e-01],\n",
      "        [-2.0191e-01, -2.3396e-01,  8.2445e-02,  1.8486e-01],\n",
      "        [ 1.1215e-01, -2.4097e-01,  4.8545e-01, -7.3602e-02],\n",
      "        [-3.0621e-01, -2.3386e-01,  4.9221e-01,  2.4080e-05],\n",
      "        [-6.7872e-02, -2.0808e-01, -1.3107e-01, -4.2111e-01],\n",
      "        [-3.9734e-01,  2.9264e-01,  4.2772e-01,  4.7715e-01],\n",
      "        [-3.6097e-01,  2.7043e-01, -3.0948e-01,  2.9828e-01]])\n",
      "\n",
      "blocks.4.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([ 0.3608,  0.3869,  0.3600,  0.3128,  0.0097,  0.2297, -0.1789,  0.2177,\n",
      "        -0.1607, -0.0084, -0.4352, -0.1307, -0.2629, -0.1687, -0.3193, -0.4497])\n",
      "\n",
      "blocks.4.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[ 0.0163,  0.1622,  0.2277,  0.1459, -0.1296, -0.2473,  0.0948,  0.1401,\n",
      "         -0.2146,  0.0896,  0.2114,  0.0151, -0.1506,  0.2050,  0.1068,  0.1656],\n",
      "        [-0.1691,  0.1455, -0.1708,  0.2474, -0.1059,  0.1506,  0.0501,  0.0663,\n",
      "         -0.0383,  0.1027, -0.1042, -0.2356, -0.0961,  0.1959, -0.0658,  0.0786],\n",
      "        [-0.0924,  0.1875,  0.1496,  0.0883, -0.1278, -0.2043,  0.0094, -0.1467,\n",
      "          0.2055, -0.2402,  0.1117,  0.2492,  0.1252,  0.0852, -0.2405,  0.2405],\n",
      "        [-0.0428, -0.2336,  0.2468, -0.1017, -0.0177,  0.2288, -0.1733, -0.1769,\n",
      "          0.0407, -0.0335,  0.0576, -0.2097,  0.0075, -0.1112, -0.1229, -0.2289]])\n",
      "\n",
      "blocks.4.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1326,  0.0482, -0.2114,  0.1984])\n",
      "\n",
      "blocks.4.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.4.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.4.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.4.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.5.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1508,  0.0928, -0.2936,  0.0754],\n",
      "        [ 0.4818,  0.3429, -0.3894,  0.4564]])\n",
      "\n",
      "blocks.5.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.0388,  0.2405,  0.3883,  0.4263],\n",
      "        [-0.3898,  0.4378, -0.3396,  0.0375]])\n",
      "\n",
      "blocks.5.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3494, -0.1096, -0.0227, -0.0598],\n",
      "        [-0.0790,  0.0394,  0.4932,  0.2905]])\n",
      "\n",
      "blocks.5.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.2797,  0.2001,  0.3871, -0.0231],\n",
      "        [ 0.0398,  0.1029, -0.4361, -0.4028]])\n",
      "\n",
      "blocks.5.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.0613, -0.1956, -0.0092, -0.1147],\n",
      "        [ 0.0778,  0.3253, -0.1658,  0.4004]])\n",
      "\n",
      "blocks.5.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3948, -0.3837, -0.3861, -0.4045],\n",
      "        [-0.2740, -0.1946, -0.0376, -0.1216]])\n",
      "\n",
      "blocks.5.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.2526, -0.1588, -0.1809,  0.4905],\n",
      "        [-0.1853, -0.3580,  0.2078, -0.0289],\n",
      "        [ 0.3828,  0.3124,  0.4594, -0.3662],\n",
      "        [ 0.3214,  0.4196, -0.2469,  0.4596]])\n",
      "\n",
      "blocks.5.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.3748,  0.0055,  0.2411, -0.1748])\n",
      "\n",
      "blocks.5.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.4361,  0.1264,  0.1491, -0.3268],\n",
      "        [ 0.2425, -0.4271,  0.4303,  0.4842],\n",
      "        [ 0.1361, -0.3137,  0.2433,  0.0852],\n",
      "        [ 0.1360,  0.1643,  0.3807, -0.2149],\n",
      "        [-0.1125,  0.1364,  0.0545,  0.4032],\n",
      "        [-0.2626, -0.0182,  0.0934, -0.1328],\n",
      "        [ 0.3409,  0.0547, -0.4621, -0.0542],\n",
      "        [-0.2268,  0.0486, -0.0581, -0.4960],\n",
      "        [-0.0911, -0.0479, -0.1474,  0.4594],\n",
      "        [-0.1091,  0.3212,  0.1239, -0.4221],\n",
      "        [ 0.1175,  0.4144, -0.3271, -0.3232],\n",
      "        [ 0.4894,  0.4018, -0.2789,  0.3009],\n",
      "        [-0.0640, -0.4930,  0.0376,  0.1615],\n",
      "        [-0.1500,  0.1739, -0.4276,  0.3465],\n",
      "        [ 0.4263,  0.2757,  0.0847,  0.1647],\n",
      "        [-0.3618, -0.1249, -0.0477, -0.2782]])\n",
      "\n",
      "blocks.5.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.3693,  0.3363,  0.3393, -0.4541,  0.1591,  0.2034,  0.4750,  0.2893,\n",
      "         0.4597, -0.1637,  0.3502,  0.4067, -0.4722, -0.4014,  0.1012,  0.2730])\n",
      "\n",
      "blocks.5.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[-0.1242,  0.0275, -0.0004,  0.0633, -0.1343,  0.1410,  0.1663, -0.1735,\n",
      "          0.0024,  0.0007,  0.0803,  0.2329,  0.0733, -0.0568,  0.2245, -0.0951],\n",
      "        [-0.0726,  0.0171, -0.1904,  0.0272, -0.1696,  0.0257,  0.0240,  0.0346,\n",
      "         -0.2108, -0.2374,  0.1150,  0.2144, -0.2218,  0.0926, -0.1840,  0.0157],\n",
      "        [ 0.2326,  0.1897, -0.1828,  0.1546,  0.1306, -0.0004,  0.2422, -0.0993,\n",
      "         -0.0582, -0.1264,  0.0360,  0.0662,  0.0199, -0.1407, -0.1125,  0.0850],\n",
      "        [-0.2192, -0.0389, -0.0055, -0.1013,  0.0910,  0.0161,  0.1755, -0.0222,\n",
      "         -0.1578,  0.1766,  0.0349, -0.0716, -0.1648,  0.0586, -0.2384,  0.0658]])\n",
      "\n",
      "blocks.5.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1236, -0.2193, -0.1637, -0.1797])\n",
      "\n",
      "blocks.5.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.5.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "blocks.5.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "blocks.5.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "ln_f.weight\n",
      "shape = (4,)\n",
      "tensor([1., 1., 1., 1.])\n",
      "\n",
      "ln_f.bias\n",
      "shape = (4,)\n",
      "tensor([0., 0., 0., 0.])\n",
      "\n",
      "lm_head.weight\n",
      "shape = (5, 4)\n",
      "tensor([[ 4.2025e-01,  4.2506e-01,  3.1300e-02,  4.6940e-01],\n",
      "        [-1.6715e-01, -4.8541e-01,  1.7465e-01,  1.7713e-01],\n",
      "        [ 4.9019e-01, -1.4472e-04,  3.8320e-01,  3.0648e-01],\n",
      "        [ 3.6188e-01, -4.9617e-01,  4.1173e-01, -4.2623e-01],\n",
      "        [ 4.4826e-01, -2.0720e-02, -4.4311e-01,  1.3905e-01]])\n",
      "\n",
      "lm_head.bias\n",
      "shape = (5,)\n",
      "tensor([-0.1018, -0.4491, -0.1813,  0.0516,  0.3894])\n"
     ]
    }
   ],
   "source": [
    "def collect_params(model):\n",
    "    params = {}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        params[name] = param.detach().cpu().clone()\n",
    "\n",
    "    return params\n",
    "\n",
    "def print_params(params, title, max_elems=5):\n",
    "    print(f\"\\n===== {title} =====\")\n",
    "    for name, tensor in params.items():\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"shape = {tuple(tensor.shape)}\")\n",
    "        print(tensor)\n",
    "\n",
    "\n",
    "init_params = collect_params(model)\n",
    "print_params(init_params, \"INITIALIZATION (before forward)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_cpu = model.eval()\n",
    "# xb_cpu = xb\n",
    "# yb_cpu = yb\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # embeddings\n",
    "#     tok_emb = model_cpu.token_embedding_table(xb_cpu)\n",
    "#     pos_emb = model_cpu.position_embedding_table(\n",
    "#         torch.arange(8,device=device))\n",
    "#     x1 = tok_emb + pos_emb\n",
    "\n",
    "#     # transformer blocks\n",
    "#     x2 = model_cpu.blocks(x1)\n",
    "\n",
    "#     # FINAL LayerNorm (this is what you want)\n",
    "#     x_ln = model_cpu.ln_f(x2)\n",
    "\n",
    "#     # logits\n",
    "#     logits_cpu = model_cpu.lm_head(x_ln)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717675ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6285,  0.3582,  0.4341,  1.5654, -0.0124],\n",
      "         [ 0.1954, -0.6630,  0.2420,  0.7547,  1.4802],\n",
      "         [-0.3672,  0.4535,  0.5218,  1.2911,  0.4613],\n",
      "         [ 0.0346, -0.5020,  0.3491,  1.1050,  1.2939],\n",
      "         [-0.4190,  0.5429,  0.4794,  1.1957,  0.2875],\n",
      "         [-0.6757,  0.1729,  0.3796,  1.6698, -0.0717],\n",
      "         [ 0.2339,  0.1642,  0.1994, -0.1062,  1.1169],\n",
      "         [-0.1207, -0.5863,  0.3150,  1.3329,  1.0475]],\n",
      "\n",
      "        [[-0.6285,  0.3582,  0.4341,  1.5654, -0.0124],\n",
      "         [ 0.1954, -0.6630,  0.2420,  0.7547,  1.4802],\n",
      "         [-0.3672,  0.4535,  0.5218,  1.2911,  0.4613],\n",
      "         [ 0.0346, -0.5020,  0.3491,  1.1050,  1.2939],\n",
      "         [-0.4190,  0.5429,  0.4794,  1.1957,  0.2875],\n",
      "         [-0.6757,  0.1729,  0.3796,  1.6698, -0.0717],\n",
      "         [ 0.2339,  0.1642,  0.1994, -0.1062,  1.1169],\n",
      "         [-0.1207, -0.5863,  0.3150,  1.3329,  1.0475]],\n",
      "\n",
      "        [[-0.1733, -0.0917,  0.5011,  1.4163,  0.9796],\n",
      "         [-0.0977, -0.8045,  0.1918,  1.2022,  1.0250],\n",
      "         [ 0.0820,  0.3432,  0.0183, -0.4444,  0.5726],\n",
      "         [-0.0913, -0.5915,  0.3147,  1.2921,  1.0949],\n",
      "         [-0.5150,  0.4293,  0.4947,  1.4723,  0.2085],\n",
      "         [-0.3562,  0.1008,  0.5290,  1.5916,  0.6449],\n",
      "         [-0.3960,  0.5061,  0.5009,  1.2417,  0.3668],\n",
      "         [ 0.1010, -0.6889,  0.2548,  0.9485,  1.3613]],\n",
      "\n",
      "        [[-0.5788,  0.0289,  0.4228,  1.7453,  0.1975],\n",
      "         [ 0.0650, -0.5467,  0.3256,  1.0436,  1.3313],\n",
      "         [ 0.3408, -0.1176, -0.2797, -1.1236,  0.8042],\n",
      "         [-0.2093, -0.5171,  0.3376,  1.4591,  0.9074],\n",
      "         [-0.5393,  0.5528,  0.4308,  1.2575,  0.0350],\n",
      "         [-0.5015, -0.0953,  0.4259,  1.7419,  0.3768],\n",
      "         [-0.0383,  0.2531,  0.4835,  0.8785,  1.0129],\n",
      "         [ 0.2759, -0.9222,  0.0821,  0.4887,  1.5193]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print(logits_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7e252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "9ea01ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.0087, val loss1.9599\n",
      "step 100: train loss 1.5707, val loss1.5766\n",
      "step 200: train loss 1.2793, val loss1.2958\n",
      "step 300: train loss 0.8688, val loss0.8739\n",
      "step 400: train loss 0.6385, val loss0.6297\n",
      "step 500: train loss 0.5086, val loss0.4802\n",
      "step 600: train loss 0.3719, val loss0.3793\n",
      "step 700: train loss 0.2736, val loss0.2810\n",
      "step 800: train loss 0.2007, val loss0.2114\n",
      "step 900: train loss 0.1412, val loss0.1356\n",
      "0.1377522349357605\n"
     ]
    }
   ],
   "source": [
    "#batch_size=32\n",
    "for iter in range(max_iters):\n",
    "\n",
    "\n",
    "\n",
    "    # sample a batch of data\n",
    "    (xb,yb)=get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits,loss=model(xb,yb)\n",
    "\n",
    "    #  capture right before FIRST optimizer update\n",
    "    if iter == 0:\n",
    "        first_batch=(xb,yb)\n",
    "        logits_before_opt = logits.detach().cpu().clone()\n",
    "        \n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #  capture right after FIRST optimizer update\n",
    "    if iter == 0:\n",
    "        first_step_params = collect_params(model)\n",
    "        logits_initial = logits.detach().cpu().clone()\n",
    "\n",
    "    # evey once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval==0:\n",
    "        losses=estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss{losses['val']:.4f}\")\n",
    "        \n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "10fef542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== AFTER 1st OPTIMIZER STEP =====\n",
      "\n",
      "token_embedding_table.weight\n",
      "shape = (5, 4)\n",
      "tensor([[ 0.6774, -1.2355, -0.0421, -1.6037],\n",
      "        [ 0.3549, -0.6876, -0.4924,  0.2425],\n",
      "        [-1.1099,  0.0905, -2.3159, -0.2178],\n",
      "        [-0.3087, -0.3947,  0.8024, -0.6206],\n",
      "        [-0.5930, -0.0621, -0.8295,  0.3299]])\n",
      "\n",
      "position_embedding_table.weight\n",
      "shape = (8, 4)\n",
      "tensor([[ 0.0339,  0.3221,  1.5746, -0.8445],\n",
      "        [ 1.3133,  0.6862, -1.0882, -0.3563],\n",
      "        [-1.4190,  0.8953,  0.0489,  2.2677],\n",
      "        [ 1.1799, -0.4335, -1.3874, -1.2872],\n",
      "        [-0.8381, -0.9234,  1.8123,  0.1616],\n",
      "        [ 0.3662,  0.1744,  1.3861, -0.4448],\n",
      "        [-1.2014,  0.7068, -1.0749,  0.5366],\n",
      "        [ 1.1764,  0.5602, -0.4537, -0.7708]])\n",
      "\n",
      "blocks.0.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4821, -0.4116, -0.4969, -0.3902],\n",
      "        [-0.3353,  0.2035,  0.1780,  0.4145]])\n",
      "\n",
      "blocks.0.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2592, -0.3419,  0.2663, -0.2031],\n",
      "        [ 0.3025, -0.1176,  0.2850, -0.3875]])\n",
      "\n",
      "blocks.0.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2513,  0.1514,  0.1067, -0.1285],\n",
      "        [ 0.2990,  0.3389, -0.3616, -0.2679]])\n",
      "\n",
      "blocks.0.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4588, -0.1677, -0.1783, -0.4828],\n",
      "        [-0.2873,  0.1239, -0.0650, -0.3619]])\n",
      "\n",
      "blocks.0.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.0107, -0.3405, -0.4252, -0.2743],\n",
      "        [-0.4386, -0.3194,  0.5008,  0.0934]])\n",
      "\n",
      "blocks.0.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1551, -0.4653, -0.3294, -0.1674],\n",
      "        [ 0.0792, -0.4390, -0.2164, -0.3003]])\n",
      "\n",
      "blocks.0.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[ 3.8562e-04, -1.8705e-01, -3.3648e-02, -3.3781e-01],\n",
      "        [-3.4419e-01, -2.9270e-01, -1.7215e-01, -3.9564e-01],\n",
      "        [ 4.2023e-01, -9.8231e-02,  4.3119e-01,  1.5679e-01],\n",
      "        [-4.2239e-01,  3.4701e-01, -1.3857e-01, -1.9066e-01]])\n",
      "\n",
      "blocks.0.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([-0.4160, -0.4981,  0.1441, -0.1082])\n",
      "\n",
      "blocks.0.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[ 0.1937, -0.4113,  0.3722, -0.3660],\n",
      "        [-0.0853,  0.1054,  0.2571,  0.4027],\n",
      "        [ 0.4565, -0.3975,  0.1268, -0.2161],\n",
      "        [-0.0538, -0.3732,  0.4544, -0.3660],\n",
      "        [ 0.2662,  0.1747,  0.1635, -0.2713],\n",
      "        [ 0.4555,  0.1089,  0.0653, -0.4416],\n",
      "        [ 0.2089, -0.0740, -0.2301,  0.4305],\n",
      "        [ 0.1125, -0.2756, -0.2541, -0.0229],\n",
      "        [ 0.2802, -0.1268, -0.2863, -0.1722],\n",
      "        [-0.3745,  0.1773,  0.3880, -0.4717],\n",
      "        [ 0.1171,  0.2573,  0.0917, -0.1791],\n",
      "        [ 0.2620,  0.2638,  0.1860, -0.0869],\n",
      "        [-0.1314,  0.0545, -0.0893, -0.1480],\n",
      "        [ 0.3186,  0.4307, -0.0505, -0.1109],\n",
      "        [ 0.0083, -0.0289,  0.1192,  0.1411],\n",
      "        [-0.4531, -0.1835,  0.4201,  0.1958]])\n",
      "\n",
      "blocks.0.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.0239, -0.3024, -0.3049, -0.4489, -0.1620,  0.1699,  0.3198,  0.2318,\n",
      "        -0.4410, -0.2997, -0.0779,  0.4827,  0.0733, -0.1285,  0.2059, -0.1914])\n",
      "\n",
      "blocks.0.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[-0.1628,  0.1815, -0.1147, -0.0511, -0.2497,  0.1663,  0.1904,  0.0901,\n",
      "         -0.1733, -0.2477, -0.2020,  0.1854,  0.1210,  0.2114,  0.1300,  0.0623],\n",
      "        [-0.0034, -0.1911, -0.2152, -0.2348,  0.1013, -0.1237, -0.0513, -0.1449,\n",
      "         -0.0466, -0.1770, -0.1644,  0.0819, -0.0753,  0.1533, -0.0812, -0.1844],\n",
      "        [-0.0431, -0.1222, -0.0755, -0.2370,  0.1389, -0.1730,  0.1267,  0.1144,\n",
      "          0.1796, -0.1928,  0.1788, -0.1172,  0.0938,  0.2358, -0.0343, -0.0029],\n",
      "        [-0.0566, -0.2077,  0.1210, -0.2472,  0.1562,  0.1881,  0.2354, -0.0580,\n",
      "         -0.2064,  0.0572,  0.1391, -0.2478, -0.0557, -0.1509, -0.0209, -0.1221]])\n",
      "\n",
      "blocks.0.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.1032, -0.0804, -0.2366,  0.2061])\n",
      "\n",
      "blocks.0.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([1.0010, 0.9990, 1.0010, 1.0010])\n",
      "\n",
      "blocks.0.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0010,  0.0010, -0.0010, -0.0010])\n",
      "\n",
      "blocks.0.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.0010, 1.0010, 1.0010, 1.0010])\n",
      "\n",
      "blocks.0.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0010, -0.0010, -0.0010, -0.0010])\n",
      "\n",
      "blocks.1.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4182, -0.0774, -0.0559, -0.2031],\n",
      "        [-0.4525, -0.4856,  0.1848, -0.2735]])\n",
      "\n",
      "blocks.1.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3204, -0.0400, -0.1675, -0.1608],\n",
      "        [ 0.0151, -0.1051, -0.1712, -0.2404]])\n",
      "\n",
      "blocks.1.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4079,  0.4202, -0.2011,  0.1335],\n",
      "        [-0.1745,  0.0416,  0.4651,  0.2314]])\n",
      "\n",
      "blocks.1.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4323,  0.1974,  0.4736,  0.1305],\n",
      "        [ 0.3342,  0.4939, -0.0776,  0.1048]])\n",
      "\n",
      "blocks.1.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3465, -0.1040,  0.3693,  0.2573],\n",
      "        [-0.3174, -0.3999, -0.3407, -0.4944]])\n",
      "\n",
      "blocks.1.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3848, -0.1246,  0.3384,  0.0827],\n",
      "        [-0.3793, -0.4021,  0.2497, -0.3709]])\n",
      "\n",
      "blocks.1.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.0606,  0.2389, -0.2324, -0.0555],\n",
      "        [-0.0425, -0.1173, -0.2525, -0.4447],\n",
      "        [-0.4052, -0.2667,  0.4819, -0.2405],\n",
      "        [-0.3368,  0.1222,  0.1388,  0.2750]])\n",
      "\n",
      "blocks.1.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.3791,  0.2774, -0.4947,  0.0453])\n",
      "\n",
      "blocks.1.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[ 0.3039, -0.0452, -0.2956,  0.4777],\n",
      "        [-0.1860, -0.2837, -0.4518,  0.0213],\n",
      "        [ 0.2206,  0.1097,  0.0999, -0.3782],\n",
      "        [-0.4659,  0.0098,  0.4549,  0.2875],\n",
      "        [-0.2921, -0.0659, -0.3696, -0.2402],\n",
      "        [ 0.0915,  0.2733,  0.4132, -0.4600],\n",
      "        [ 0.3333, -0.3516,  0.1882,  0.4241],\n",
      "        [ 0.0060,  0.4559, -0.4270, -0.1900],\n",
      "        [ 0.2926, -0.1079, -0.1013, -0.2094],\n",
      "        [ 0.3436,  0.2442,  0.1612, -0.2820],\n",
      "        [-0.4069,  0.0531,  0.1491, -0.2299],\n",
      "        [-0.1389,  0.3387,  0.0388,  0.0236],\n",
      "        [-0.1220, -0.4518, -0.4711, -0.2400],\n",
      "        [-0.2552,  0.1548, -0.1446, -0.1946],\n",
      "        [ 0.4777,  0.1752,  0.3554, -0.2431],\n",
      "        [-0.2052,  0.1848, -0.3341, -0.3258]])\n",
      "\n",
      "blocks.1.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.0231, -0.1819, -0.3758,  0.2956,  0.4031,  0.0821, -0.0861, -0.4621,\n",
      "        -0.1831,  0.1263,  0.2368, -0.0622, -0.1967,  0.2776, -0.3972,  0.3150])\n",
      "\n",
      "blocks.1.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[-0.0960,  0.0048, -0.0484,  0.0293, -0.0745,  0.1828, -0.0075,  0.1961,\n",
      "          0.2414, -0.1208, -0.1834,  0.2016,  0.1949, -0.1899, -0.0183, -0.2455],\n",
      "        [-0.2056,  0.0473,  0.0675,  0.0520, -0.0690,  0.2316,  0.0347, -0.1485,\n",
      "         -0.0132,  0.0610,  0.0885, -0.1778,  0.0927, -0.1287, -0.2067, -0.1376],\n",
      "        [ 0.2421,  0.2147,  0.2229,  0.1458,  0.1899, -0.0345, -0.1366,  0.1259,\n",
      "         -0.1305, -0.1697, -0.0808,  0.0535,  0.1297, -0.0961, -0.1481,  0.0347],\n",
      "        [-0.1484, -0.1638,  0.1313, -0.0410,  0.2274,  0.2442,  0.0758,  0.0850,\n",
      "          0.0586,  0.0049, -0.0172,  0.0044,  0.0924,  0.2314, -0.0638, -0.1067]])\n",
      "\n",
      "blocks.1.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0615, -0.1218,  0.0435,  0.1876])\n",
      "\n",
      "blocks.1.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([0.9990, 1.0010, 1.0010, 1.0010])\n",
      "\n",
      "blocks.1.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0010, -0.0010,  0.0010, -0.0010])\n",
      "\n",
      "blocks.1.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.0010, 1.0010, 1.0010, 0.9990])\n",
      "\n",
      "blocks.1.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0010,  0.0010, -0.0010,  0.0010])\n",
      "\n",
      "blocks.2.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3900,  0.2286, -0.3670, -0.2673],\n",
      "        [-0.1109, -0.0912,  0.0401, -0.4580]])\n",
      "\n",
      "blocks.2.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1546, -0.3824, -0.3154, -0.4167],\n",
      "        [ 0.4367, -0.4725,  0.3762, -0.0158]])\n",
      "\n",
      "blocks.2.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.0591,  0.3117, -0.0452,  0.3146],\n",
      "        [ 0.3625, -0.4331,  0.1914,  0.0954]])\n",
      "\n",
      "blocks.2.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1065,  0.0720,  0.1378, -0.2415],\n",
      "        [-0.0650,  0.4741,  0.3369, -0.0198]])\n",
      "\n",
      "blocks.2.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4713,  0.0229, -0.3415,  0.4056],\n",
      "        [-0.3025, -0.0371, -0.1100,  0.0900]])\n",
      "\n",
      "blocks.2.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4695,  0.0485,  0.2886,  0.3871],\n",
      "        [ 0.4027, -0.1717, -0.1108,  0.2400]])\n",
      "\n",
      "blocks.2.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.1354,  0.2331, -0.1102, -0.3381],\n",
      "        [ 0.2045,  0.0757,  0.2219,  0.4957],\n",
      "        [ 0.3404,  0.4750,  0.0278, -0.4291],\n",
      "        [-0.3518, -0.3096, -0.4396, -0.2496]])\n",
      "\n",
      "blocks.2.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([-0.4613, -0.4623, -0.2978, -0.4919])\n",
      "\n",
      "blocks.2.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.3079,  0.1897,  0.4180, -0.1477],\n",
      "        [-0.1464,  0.2680, -0.2477, -0.2354],\n",
      "        [ 0.3071, -0.4346,  0.0601,  0.4407],\n",
      "        [ 0.0867,  0.1370, -0.2922, -0.0079],\n",
      "        [ 0.0265,  0.1217,  0.1953,  0.4355],\n",
      "        [-0.3816,  0.0150, -0.2498, -0.3955],\n",
      "        [-0.0390, -0.4411,  0.3499,  0.0589],\n",
      "        [-0.2705,  0.2623, -0.4742, -0.1924],\n",
      "        [-0.0984, -0.4239, -0.3189, -0.0826],\n",
      "        [ 0.3784,  0.4818,  0.3171, -0.2976],\n",
      "        [-0.3281,  0.4353,  0.1759,  0.0143],\n",
      "        [ 0.0687, -0.4008, -0.1679,  0.4823],\n",
      "        [-0.1243, -0.0241, -0.4162, -0.2787],\n",
      "        [-0.0092, -0.3096, -0.0630,  0.2025],\n",
      "        [-0.4901,  0.1475, -0.3296, -0.2430],\n",
      "        [ 0.1910,  0.3966, -0.1377, -0.2043]])\n",
      "\n",
      "blocks.2.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.4511, -0.2568, -0.4388, -0.1134,  0.1010, -0.4684,  0.4376,  0.3147,\n",
      "        -0.4905, -0.2398,  0.1621, -0.1017, -0.0555, -0.2268,  0.4026, -0.2805])\n",
      "\n",
      "blocks.2.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[ 0.2063,  0.0171,  0.0493,  0.1960, -0.0422, -0.1423, -0.0414,  0.2038,\n",
      "         -0.1865,  0.0577, -0.2447,  0.1301,  0.0934,  0.0096,  0.1083,  0.0013],\n",
      "        [ 0.1373, -0.1989, -0.0377,  0.1099,  0.2480,  0.1273, -0.1828,  0.1913,\n",
      "         -0.0567, -0.0524, -0.2262, -0.0404,  0.1758,  0.0339, -0.1466,  0.0780],\n",
      "        [-0.0812,  0.2292, -0.2160, -0.0780, -0.2424, -0.0985,  0.0798,  0.2417,\n",
      "          0.0410,  0.2441,  0.0479,  0.1454,  0.2014,  0.2080, -0.1389,  0.2288],\n",
      "        [ 0.1524, -0.1179, -0.1183, -0.2107,  0.0638, -0.2026,  0.1066,  0.0779,\n",
      "         -0.2162,  0.0691, -0.0193,  0.1152,  0.1424, -0.2475,  0.2283,  0.2087]])\n",
      "\n",
      "blocks.2.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0985, -0.2295, -0.0883, -0.0715])\n",
      "\n",
      "blocks.2.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([0.9990, 0.9990, 0.9990, 0.9990])\n",
      "\n",
      "blocks.2.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0010,  0.0010,  0.0010, -0.0010])\n",
      "\n",
      "blocks.2.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.0010, 0.9990, 1.0010, 0.9990])\n",
      "\n",
      "blocks.2.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0010, -0.0010, -0.0010, -0.0010])\n",
      "\n",
      "blocks.3.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1295,  0.2810,  0.1828,  0.3971],\n",
      "        [-0.1863,  0.1673,  0.1769, -0.4173]])\n",
      "\n",
      "blocks.3.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4840, -0.2604,  0.3433, -0.4697],\n",
      "        [-0.4362,  0.2791,  0.2708,  0.4102]])\n",
      "\n",
      "blocks.3.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3765, -0.3669,  0.2575,  0.4358],\n",
      "        [ 0.3002,  0.0793,  0.1638,  0.4736]])\n",
      "\n",
      "blocks.3.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3236, -0.2260,  0.3487, -0.3411],\n",
      "        [-0.2747,  0.3640,  0.1588,  0.1605]])\n",
      "\n",
      "blocks.3.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2129, -0.0079,  0.4586, -0.2991],\n",
      "        [ 0.0029,  0.2368, -0.3442,  0.4866]])\n",
      "\n",
      "blocks.3.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2488, -0.1191, -0.1363, -0.3248],\n",
      "        [-0.4896,  0.2809,  0.1318, -0.4673]])\n",
      "\n",
      "blocks.3.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.3228,  0.4932,  0.1901,  0.2016],\n",
      "        [-0.3001, -0.2202, -0.0745, -0.0904],\n",
      "        [-0.3416,  0.0402,  0.0487, -0.0643],\n",
      "        [ 0.0703, -0.1972,  0.1291,  0.1876]])\n",
      "\n",
      "blocks.3.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([-0.2644, -0.4968,  0.2627,  0.1203])\n",
      "\n",
      "blocks.3.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.2553,  0.4828, -0.2271,  0.3389],\n",
      "        [ 0.2547, -0.4202,  0.3235, -0.4587],\n",
      "        [-0.2760, -0.0824, -0.3380,  0.4875],\n",
      "        [-0.0993,  0.1996, -0.4475,  0.2888],\n",
      "        [-0.1564, -0.3793,  0.0721,  0.2432],\n",
      "        [ 0.4337, -0.3064, -0.2470,  0.0971],\n",
      "        [ 0.1346,  0.1912,  0.2754, -0.1144],\n",
      "        [ 0.2768,  0.3696, -0.1316,  0.3567],\n",
      "        [ 0.2453,  0.4400, -0.2831, -0.2479],\n",
      "        [-0.1436,  0.0244,  0.3011, -0.2864],\n",
      "        [ 0.2493, -0.1782,  0.3031, -0.0247],\n",
      "        [-0.4370, -0.2761, -0.3629,  0.2470],\n",
      "        [-0.3343, -0.0407,  0.1069, -0.2752],\n",
      "        [ 0.1452, -0.4872, -0.3588, -0.4521],\n",
      "        [-0.1502, -0.1831,  0.0706, -0.0915],\n",
      "        [ 0.2360,  0.4574,  0.1784, -0.1980]])\n",
      "\n",
      "blocks.3.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.4672,  0.1821, -0.2487,  0.2554,  0.3332,  0.1939,  0.4682,  0.4739,\n",
      "         0.1049, -0.3633,  0.4457, -0.2382, -0.2372,  0.4194,  0.3884,  0.1521])\n",
      "\n",
      "blocks.3.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[ 0.0167, -0.2113, -0.0250,  0.2408,  0.0627,  0.0204, -0.0529, -0.0862,\n",
      "          0.1500,  0.0144,  0.1616, -0.0432,  0.1082,  0.1022,  0.0389,  0.1581],\n",
      "        [ 0.1557,  0.2327,  0.1912, -0.0649, -0.2127,  0.0447, -0.0012, -0.0662,\n",
      "         -0.0429,  0.0128,  0.1834,  0.0769, -0.0898, -0.1018, -0.0629, -0.0956],\n",
      "        [ 0.2258,  0.1314,  0.2267,  0.0018,  0.0494,  0.0877, -0.2376,  0.0233,\n",
      "         -0.0157, -0.1412, -0.1950,  0.2203,  0.2033,  0.1149,  0.2376, -0.1025],\n",
      "        [-0.0442,  0.0957, -0.0403, -0.0501, -0.2056,  0.0682, -0.1501,  0.0081,\n",
      "          0.2427, -0.0760, -0.0778,  0.1518, -0.0909, -0.0205,  0.2344, -0.1035]])\n",
      "\n",
      "blocks.3.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.1799, -0.1409, -0.0703, -0.1176])\n",
      "\n",
      "blocks.3.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([0.9990, 1.0010, 1.0010, 0.9990])\n",
      "\n",
      "blocks.3.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0010, -0.0010,  0.0010, -0.0010])\n",
      "\n",
      "blocks.3.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([0.9990, 0.9990, 1.0010, 0.9990])\n",
      "\n",
      "blocks.3.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0010,  0.0010,  0.0010, -0.0010])\n",
      "\n",
      "blocks.4.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2585,  0.2030,  0.0840, -0.1610],\n",
      "        [-0.3875, -0.1564, -0.2120, -0.1617]])\n",
      "\n",
      "blocks.4.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4521,  0.1067, -0.3664, -0.3884],\n",
      "        [-0.4095,  0.2077, -0.3000, -0.2054]])\n",
      "\n",
      "blocks.4.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3909,  0.2665,  0.2877, -0.4757],\n",
      "        [-0.3575, -0.1898,  0.4140,  0.0521]])\n",
      "\n",
      "blocks.4.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3729,  0.0041, -0.3893, -0.1105],\n",
      "        [-0.1365,  0.4338,  0.1539, -0.0882]])\n",
      "\n",
      "blocks.4.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.0835, -0.1453,  0.1954,  0.1988],\n",
      "        [ 0.1333, -0.1939,  0.4256, -0.0712]])\n",
      "\n",
      "blocks.4.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1937,  0.3142,  0.4085,  0.4966],\n",
      "        [ 0.1471, -0.1694,  0.2529,  0.4300]])\n",
      "\n",
      "blocks.4.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.4914, -0.0629, -0.3420,  0.0922],\n",
      "        [ 0.2058, -0.1023, -0.0408,  0.2241],\n",
      "        [-0.0830, -0.4209,  0.3991, -0.2527],\n",
      "        [-0.0539,  0.0482, -0.0290, -0.4693]])\n",
      "\n",
      "blocks.4.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.2304, -0.2281, -0.2603,  0.1205])\n",
      "\n",
      "blocks.4.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.2619, -0.2321, -0.1695, -0.1868],\n",
      "        [-0.2098, -0.1338,  0.1289, -0.4056],\n",
      "        [-0.3036,  0.0063,  0.0705,  0.2771],\n",
      "        [-0.3502,  0.1606,  0.2832,  0.2766],\n",
      "        [-0.4667, -0.1918, -0.4288, -0.3174],\n",
      "        [ 0.2775, -0.0757,  0.2134, -0.2945],\n",
      "        [ 0.0750, -0.3014,  0.2489, -0.2177],\n",
      "        [-0.1264, -0.4328,  0.0007,  0.4757],\n",
      "        [ 0.2417, -0.2678,  0.0077, -0.0538],\n",
      "        [-0.4035,  0.3910,  0.0091,  0.1063],\n",
      "        [-0.2029, -0.2330,  0.0814,  0.1859],\n",
      "        [ 0.1111, -0.2400,  0.4844, -0.0746],\n",
      "        [-0.3052, -0.2349,  0.4932, -0.0010],\n",
      "        [-0.0689, -0.2071, -0.1321, -0.4201],\n",
      "        [-0.3983,  0.2936,  0.4267,  0.4781],\n",
      "        [-0.3620,  0.2714, -0.3105,  0.2993]])\n",
      "\n",
      "blocks.4.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([ 0.3618,  0.3859,  0.3610,  0.3118,  0.0087,  0.2307, -0.1799,  0.2167,\n",
      "        -0.1597, -0.0074, -0.4342, -0.1317, -0.2619, -0.1697, -0.3183, -0.4487])\n",
      "\n",
      "blocks.4.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[ 0.0153,  0.1612,  0.2287,  0.1469, -0.1286, -0.2482,  0.0938,  0.1391,\n",
      "         -0.2156,  0.0906,  0.2104,  0.0141, -0.1516,  0.2040,  0.1078,  0.1665],\n",
      "        [-0.1701,  0.1445, -0.1718,  0.2464, -0.1069,  0.1496,  0.0491,  0.0653,\n",
      "         -0.0393,  0.1017, -0.1052, -0.2366, -0.0970,  0.1969, -0.0668,  0.0776],\n",
      "        [-0.0934,  0.1865,  0.1486,  0.0873, -0.1268, -0.2053,  0.0084, -0.1477,\n",
      "          0.2045, -0.2392,  0.1107,  0.2482,  0.1242,  0.0842, -0.2415,  0.2415],\n",
      "        [-0.0418, -0.2326,  0.2478, -0.1007, -0.0187,  0.2298, -0.1723, -0.1759,\n",
      "          0.0417, -0.0345,  0.0586, -0.2087,  0.0085, -0.1102, -0.1219, -0.2299]])\n",
      "\n",
      "blocks.4.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1316,  0.0472, -0.2124,  0.1994])\n",
      "\n",
      "blocks.4.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([0.9990, 1.0010, 1.0010, 0.9990])\n",
      "\n",
      "blocks.4.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0010, -0.0010, -0.0010, -0.0010])\n",
      "\n",
      "blocks.4.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.0010, 0.9990, 0.9990, 1.0010])\n",
      "\n",
      "blocks.4.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0010,  0.0010, -0.0010,  0.0010])\n",
      "\n",
      "blocks.5.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1498,  0.0918, -0.2926,  0.0764],\n",
      "        [ 0.4828,  0.3439, -0.3904,  0.4554]])\n",
      "\n",
      "blocks.5.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.0378,  0.2415,  0.3873,  0.4253],\n",
      "        [-0.3908,  0.4388, -0.3406,  0.0365]])\n",
      "\n",
      "blocks.5.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3484, -0.1106, -0.0217, -0.0608],\n",
      "        [-0.0780,  0.0384,  0.4942,  0.2895]])\n",
      "\n",
      "blocks.5.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.2787,  0.1991,  0.3881, -0.0221],\n",
      "        [ 0.0408,  0.1039, -0.4371, -0.4038]])\n",
      "\n",
      "blocks.5.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.0603, -0.1946, -0.0082, -0.1137],\n",
      "        [ 0.0768,  0.3263, -0.1668,  0.3994]])\n",
      "\n",
      "blocks.5.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3938, -0.3827, -0.3871, -0.4055],\n",
      "        [-0.2730, -0.1956, -0.0366, -0.1206]])\n",
      "\n",
      "blocks.5.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.2536, -0.1598, -0.1799,  0.4895],\n",
      "        [-0.1843, -0.3590,  0.2068, -0.0279],\n",
      "        [ 0.3818,  0.3114,  0.4604, -0.3672],\n",
      "        [ 0.3204,  0.4206, -0.2459,  0.4606]])\n",
      "\n",
      "blocks.5.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.3758,  0.0045,  0.2401, -0.1738])\n",
      "\n",
      "blocks.5.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.4361,  0.1264,  0.1491, -0.3268],\n",
      "        [ 0.2415, -0.4261,  0.4293,  0.4852],\n",
      "        [ 0.1371, -0.3147,  0.2443,  0.0842],\n",
      "        [ 0.1350,  0.1653,  0.3797, -0.2139],\n",
      "        [-0.1135,  0.1354,  0.0555,  0.4042],\n",
      "        [-0.2616, -0.0192,  0.0924, -0.1318],\n",
      "        [ 0.3419,  0.0537, -0.4631, -0.0532],\n",
      "        [-0.2278,  0.0476, -0.0571, -0.4970],\n",
      "        [-0.0901, -0.0469, -0.1484,  0.4584],\n",
      "        [-0.1081,  0.3202,  0.1249, -0.4231],\n",
      "        [ 0.1165,  0.4154, -0.3280, -0.3242],\n",
      "        [ 0.4904,  0.4028, -0.2799,  0.2999],\n",
      "        [-0.0650, -0.4920,  0.0366,  0.1605],\n",
      "        [-0.1510,  0.1729, -0.4265,  0.3455],\n",
      "        [ 0.4273,  0.2767,  0.0837,  0.1637],\n",
      "        [-0.3628, -0.1259, -0.0467, -0.2772]])\n",
      "\n",
      "blocks.5.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.3693,  0.3353,  0.3403, -0.4551,  0.1601,  0.2044,  0.4760,  0.2883,\n",
      "         0.4607, -0.1627,  0.3512,  0.4077, -0.4732, -0.4023,  0.1022,  0.2720])\n",
      "\n",
      "blocks.5.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[-0.1242,  0.0265, -0.0014,  0.0643, -0.1333,  0.1420,  0.1673, -0.1725,\n",
      "          0.0034,  0.0017,  0.0813,  0.2339,  0.0743, -0.0558,  0.2255, -0.0941],\n",
      "        [-0.0726,  0.0161, -0.1914,  0.0282, -0.1706,  0.0247,  0.0230,  0.0336,\n",
      "         -0.2118, -0.2364,  0.1140,  0.2134, -0.2228,  0.0916, -0.1850,  0.0147],\n",
      "        [ 0.2326,  0.1887, -0.1838,  0.1536,  0.1316, -0.0014,  0.2432, -0.1003,\n",
      "         -0.0572, -0.1274,  0.0370,  0.0672,  0.0189, -0.1397, -0.1115,  0.0840],\n",
      "        [-0.2192, -0.0379, -0.0045, -0.1003,  0.0920,  0.0171,  0.1745, -0.0212,\n",
      "         -0.1568,  0.1776,  0.0339, -0.0726, -0.1638,  0.0576, -0.2394,  0.0668]])\n",
      "\n",
      "blocks.5.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1246, -0.2203, -0.1647, -0.1787])\n",
      "\n",
      "blocks.5.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([0.9990, 1.0010, 1.0010, 1.0010])\n",
      "\n",
      "blocks.5.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0010,  0.0010,  0.0010,  0.0010])\n",
      "\n",
      "blocks.5.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.0010, 1.0010, 0.9990, 1.0010])\n",
      "\n",
      "blocks.5.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0010,  0.0010, -0.0010, -0.0010])\n",
      "\n",
      "ln_f.weight\n",
      "shape = (4,)\n",
      "tensor([0.9990, 0.9990, 0.9990, 0.9990])\n",
      "\n",
      "ln_f.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0010,  0.0010,  0.0010,  0.0010])\n",
      "\n",
      "lm_head.weight\n",
      "shape = (5, 4)\n",
      "tensor([[ 0.4212,  0.4241,  0.0323,  0.4684],\n",
      "        [-0.1662, -0.4864,  0.1756,  0.1761],\n",
      "        [ 0.4912, -0.0011,  0.3842,  0.3055],\n",
      "        [ 0.3609, -0.4952,  0.4107, -0.4252],\n",
      "        [ 0.4473, -0.0217, -0.4421,  0.1400]])\n",
      "\n",
      "lm_head.bias\n",
      "shape = (5,)\n",
      "tensor([-0.1008, -0.4481, -0.1803,  0.0506,  0.3884])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_params(first_step_params, \"AFTER 1st OPTIMIZER STEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2388c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== AFTER LAST OPTIMIZER STEP =====\n",
      "\n",
      "token_embedding_table.weight\n",
      "shape = (5, 4)\n",
      "tensor([[ 1.2184, -1.9322,  0.6959, -1.5293],\n",
      "        [ 0.4196, -1.1127, -1.0876,  1.1806],\n",
      "        [-0.9501,  0.3700, -2.5186, -0.5853],\n",
      "        [-0.6675, -0.0389,  0.9222, -0.6924],\n",
      "        [-0.6536, -0.2631, -0.6040,  0.5524]])\n",
      "\n",
      "position_embedding_table.weight\n",
      "shape = (8, 4)\n",
      "tensor([[ 0.0548,  0.3112,  1.4023, -0.5806],\n",
      "        [ 1.2468,  0.5932, -0.8611, -0.4066],\n",
      "        [-1.1272,  0.8496, -0.0511,  2.0627],\n",
      "        [ 0.8585, -0.3408, -1.2980, -1.3582],\n",
      "        [-0.7949, -0.4111,  1.3308, -0.1955],\n",
      "        [ 0.2491,  0.2504,  1.2860, -0.3715],\n",
      "        [-1.0116,  0.2084, -0.5759,  0.7111],\n",
      "        [ 1.2096,  0.4021, -0.4463, -0.6036]])\n",
      "\n",
      "blocks.0.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4412, -0.3431, -0.4679, -0.4997],\n",
      "        [-0.3011,  0.3035,  0.0745,  0.4491]])\n",
      "\n",
      "blocks.0.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1766, -0.3070,  0.2269, -0.2699],\n",
      "        [ 0.2770, -0.0716,  0.3096, -0.4571]])\n",
      "\n",
      "blocks.0.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1945,  0.0388,  0.1776, -0.0969],\n",
      "        [ 0.1685,  0.2068, -0.2110, -0.1970]])\n",
      "\n",
      "blocks.0.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.6754, -0.3735, -0.2779, -0.6382],\n",
      "        [-0.2892,  0.0147,  0.0826, -0.3562]])\n",
      "\n",
      "blocks.0.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.2035, -0.5529, -0.3389, -0.3413],\n",
      "        [-0.6018, -0.0117,  0.4422,  0.0275]])\n",
      "\n",
      "blocks.0.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1140, -0.4430, -0.2681, -0.2056],\n",
      "        [ 0.0023, -0.4798, -0.1401, -0.2474]])\n",
      "\n",
      "blocks.0.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[ 0.0450, -0.2396,  0.0395, -0.2678],\n",
      "        [-0.3377, -0.0776, -0.2146, -0.4478],\n",
      "        [ 0.4211, -0.1073,  0.3535,  0.0956],\n",
      "        [-0.2489,  0.1801, -0.0610, -0.0979]])\n",
      "\n",
      "blocks.0.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([-0.2133, -0.3276,  0.1805, -0.1019])\n",
      "\n",
      "blocks.0.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[ 0.2943, -0.2512,  0.3450, -0.5903],\n",
      "        [-0.0050,  0.0103,  0.1106,  0.4853],\n",
      "        [ 0.4265, -0.4306,  0.1816, -0.1519],\n",
      "        [ 0.0365, -0.3505,  0.2630, -0.1947],\n",
      "        [ 0.5166,  0.0933,  0.0901, -0.3675],\n",
      "        [ 0.3679,  0.0214,  0.1040, -0.2450],\n",
      "        [ 0.2829, -0.2695, -0.1902,  0.4906],\n",
      "        [ 0.1444, -0.3489, -0.2639,  0.0369],\n",
      "        [ 0.5027, -0.0315, -0.4305, -0.3856],\n",
      "        [-0.3821,  0.2219,  0.3481, -0.4007],\n",
      "        [ 0.2842,  0.0845,  0.1500, -0.2262],\n",
      "        [ 0.0423,  0.4797,  0.1363, -0.0130],\n",
      "        [-0.2790,  0.2308, -0.2074, -0.0118],\n",
      "        [ 0.1070,  0.6076, -0.1588,  0.0117],\n",
      "        [-0.0324, -0.1843,  0.1565,  0.2442],\n",
      "        [-0.2500, -0.5022,  0.2774,  0.3876]])\n",
      "\n",
      "blocks.0.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([ 0.0473, -0.2825, -0.3512, -0.6359, -0.0601,  0.0926,  0.3995,  0.3628,\n",
      "        -0.2409, -0.3284, -0.1290,  0.5432,  0.1906, -0.0288,  0.2881, -0.1056])\n",
      "\n",
      "blocks.0.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[-0.1226,  0.2365, -0.0576,  0.1057, -0.3778,  0.0642,  0.2540,  0.0755,\n",
      "         -0.5624, -0.1996, -0.2545,  0.1793,  0.2535,  0.2879,  0.2709,  0.1353],\n",
      "        [ 0.0253, -0.1867, -0.2054, -0.2677,  0.1497, -0.0596, -0.1323, -0.2235,\n",
      "         -0.0177, -0.0557, -0.0684,  0.1750, -0.0100,  0.1639, -0.1117, -0.1533],\n",
      "        [ 0.2010, -0.0635,  0.0755, -0.1266,  0.3333, -0.0064,  0.1958,  0.2281,\n",
      "          0.3225, -0.2649,  0.3302, -0.1195,  0.0563,  0.2030, -0.0011,  0.0943],\n",
      "        [-0.1137, -0.2393,  0.1189, -0.2375,  0.1014,  0.1692,  0.2909,  0.1335,\n",
      "         -0.0309,  0.1212, -0.0303, -0.3853, -0.3210, -0.3678, -0.0200, -0.0030]])\n",
      "\n",
      "blocks.0.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0508, -0.0309, -0.1987,  0.1455])\n",
      "\n",
      "blocks.0.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([1.0431, 1.0372, 0.9244, 1.0248])\n",
      "\n",
      "blocks.0.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0356,  0.1042,  0.0929,  0.1570])\n",
      "\n",
      "blocks.0.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.1099, 1.2732, 0.9898, 1.0988])\n",
      "\n",
      "blocks.0.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1100,  0.0130, -0.1290,  0.0034])\n",
      "\n",
      "blocks.1.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4278, -0.0350, -0.1005, -0.1605],\n",
      "        [-0.2798, -0.6925,  0.3076, -0.3615]])\n",
      "\n",
      "blocks.1.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2722, -0.1628, -0.0455, -0.2443],\n",
      "        [ 0.1229, -0.2080,  0.0414, -0.4083]])\n",
      "\n",
      "blocks.1.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4186,  0.3823, -0.2689,  0.2559],\n",
      "        [-0.1563,  0.1049,  0.3527,  0.2733]])\n",
      "\n",
      "blocks.1.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2519,  0.2538,  0.3660,  0.0769],\n",
      "        [ 0.4910,  0.5269, -0.2219,  0.2488]])\n",
      "\n",
      "blocks.1.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.0942, -0.2742,  0.2544,  0.2731],\n",
      "        [-0.6028, -0.2381, -0.2799, -0.4385]])\n",
      "\n",
      "blocks.1.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.3208, -0.0790,  0.3275, -0.0517],\n",
      "        [-0.3087, -0.3341,  0.2238, -0.4714]])\n",
      "\n",
      "blocks.1.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[ 0.0149,  0.1416, -0.2630, -0.0807],\n",
      "        [ 0.0045, -0.0981, -0.1362, -0.3598],\n",
      "        [-0.4495, -0.2535,  0.4043, -0.2173],\n",
      "        [-0.2993,  0.1262,  0.0568,  0.1742]])\n",
      "\n",
      "blocks.1.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.3258,  0.1832, -0.3709, -0.0254])\n",
      "\n",
      "blocks.1.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[ 0.3765, -0.0265, -0.3320,  0.4718],\n",
      "        [-0.2491, -0.0862, -0.5779,  0.0585],\n",
      "        [ 0.2972, -0.0787,  0.2311, -0.4339],\n",
      "        [-0.6240,  0.1815,  0.4325,  0.2605],\n",
      "        [-0.1278, -0.2152, -0.3028, -0.2734],\n",
      "        [ 0.0756,  0.3857,  0.3020, -0.4435],\n",
      "        [ 0.4559, -0.3648,  0.0483,  0.5323],\n",
      "        [-0.0430,  0.5364, -0.4778, -0.1162],\n",
      "        [ 0.1294, -0.0350, -0.0339, -0.1582],\n",
      "        [ 0.2823,  0.2596,  0.1853, -0.2768],\n",
      "        [-0.6648,  0.0905,  0.2551, -0.2166],\n",
      "        [-0.1242,  0.4315, -0.0484,  0.0643],\n",
      "        [ 0.0638, -0.4306, -0.5936, -0.1835],\n",
      "        [-0.2191,  0.0127, -0.0024, -0.2996],\n",
      "        [ 0.5379,  0.0467,  0.4239, -0.3391],\n",
      "        [-0.2386,  0.1967, -0.3899, -0.2615]])\n",
      "\n",
      "blocks.1.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([ 0.0165, -0.0823, -0.3244,  0.2391,  0.4007,  0.0077,  0.0584, -0.3963,\n",
      "        -0.3497,  0.0006,  0.2325,  0.0169, -0.1146,  0.0642, -0.3386,  0.3658])\n",
      "\n",
      "blocks.1.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[-0.1702,  0.0071, -0.1709, -0.0463, -0.0667,  0.1822, -0.1485,  0.3006,\n",
      "          0.1296, -0.1407, -0.3496,  0.2923, -0.1502, -0.1573, -0.1456, -0.1742],\n",
      "        [-0.1744,  0.0685, -0.0249,  0.2335, -0.0258,  0.2253, -0.0703, -0.1066,\n",
      "         -0.0339, -0.0140,  0.3501, -0.1048,  0.1464, -0.0121, -0.3671, -0.0968],\n",
      "        [ 0.2816,  0.2722,  0.3659,  0.0661,  0.2179, -0.0639, -0.0415,  0.1500,\n",
      "         -0.1064, -0.1334, -0.1126,  0.0292,  0.2014, -0.0883,  0.1257,  0.0588],\n",
      "        [-0.1992, -0.3002,  0.2012, -0.1284,  0.0763,  0.1981,  0.1853, -0.1015,\n",
      "          0.1265,  0.0227, -0.1743, -0.2010,  0.3024,  0.0158, -0.0650, -0.2819]])\n",
      "\n",
      "blocks.1.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.1125, -0.0778,  0.0665,  0.1153])\n",
      "\n",
      "blocks.1.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([0.9469, 0.9895, 0.9252, 1.1470])\n",
      "\n",
      "blocks.1.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0164, -0.0447,  0.0549,  0.0660])\n",
      "\n",
      "blocks.1.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.1615, 1.0930, 1.0619, 0.9905])\n",
      "\n",
      "blocks.1.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0838,  0.0018, -0.0490,  0.1489])\n",
      "\n",
      "blocks.2.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3907,  0.2652, -0.4354, -0.2308],\n",
      "        [-0.0963, -0.0291, -0.0496, -0.4647]])\n",
      "\n",
      "blocks.2.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1438, -0.5566, -0.2924, -0.2636],\n",
      "        [ 0.3572, -0.5255,  0.2799,  0.1752]])\n",
      "\n",
      "blocks.2.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1458,  0.4013, -0.0398,  0.2793],\n",
      "        [ 0.3333, -0.3905,  0.1521,  0.0944]])\n",
      "\n",
      "blocks.2.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.1281,  0.0474,  0.1776, -0.3030],\n",
      "        [-0.0782,  0.4065,  0.4322, -0.0532]])\n",
      "\n",
      "blocks.2.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4221,  0.1336, -0.4694,  0.4103],\n",
      "        [-0.1178, -0.0722, -0.2757,  0.1189]])\n",
      "\n",
      "blocks.2.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3495,  0.1527,  0.2982,  0.3187],\n",
      "        [ 0.2716, -0.1882, -0.0355,  0.2906]])\n",
      "\n",
      "blocks.2.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.1407,  0.3211,  0.0414, -0.1816],\n",
      "        [ 0.1374,  0.0861,  0.2261,  0.4666],\n",
      "        [ 0.4400,  0.4468,  0.0160, -0.4670],\n",
      "        [-0.4450, -0.2044, -0.3656, -0.1367]])\n",
      "\n",
      "blocks.2.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([-0.4248, -0.3635, -0.1839, -0.4091])\n",
      "\n",
      "blocks.2.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.4307, -0.0019,  0.5527,  0.0847],\n",
      "        [-0.1724,  0.3362, -0.3553, -0.1398],\n",
      "        [ 0.5245, -0.3345, -0.0932,  0.2488],\n",
      "        [ 0.0224,  0.2642, -0.4152,  0.0549],\n",
      "        [-0.0559,  0.0753,  0.2896,  0.4359],\n",
      "        [-0.3834,  0.0819, -0.3480, -0.3903],\n",
      "        [ 0.1710, -0.4436,  0.1954, -0.0013],\n",
      "        [-0.2901,  0.3207, -0.5952, -0.0628],\n",
      "        [ 0.1415, -0.5088, -0.2763, -0.1384],\n",
      "        [ 0.2634,  0.5247,  0.2677, -0.2026],\n",
      "        [-0.2623,  0.4455,  0.0952,  0.0142],\n",
      "        [ 0.2111, -0.5183, -0.1946,  0.4403],\n",
      "        [-0.1490,  0.0133, -0.5064, -0.1934],\n",
      "        [ 0.0348, -0.3355, -0.0386,  0.1736],\n",
      "        [-0.4352,  0.0231, -0.3314, -0.1733],\n",
      "        [ 0.2150,  0.3444, -0.1265, -0.1861]])\n",
      "\n",
      "blocks.2.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.4066, -0.1511, -0.5627, -0.0600,  0.1165, -0.3889,  0.5127,  0.3826,\n",
      "        -0.4381, -0.2698,  0.0881, -0.0127, -0.0289, -0.2268,  0.2675, -0.3168])\n",
      "\n",
      "blocks.2.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[ 0.0824,  0.0849, -0.1447,  0.2690, -0.0363, -0.1612, -0.2122,  0.2546,\n",
      "         -0.3823,  0.0323, -0.2376, -0.0128,  0.1605, -0.0532,  0.1405,  0.0483],\n",
      "        [ 0.4160, -0.1964,  0.0289,  0.1129,  0.3363,  0.1364, -0.0482,  0.1855,\n",
      "          0.1750, -0.0416, -0.1758, -0.0639,  0.1866,  0.0483, -0.1163,  0.0899],\n",
      "        [-0.2309,  0.2839, -0.3311,  0.0017, -0.2864, -0.1636,  0.0298,  0.3017,\n",
      "         -0.4637,  0.1507, -0.0141,  0.2595,  0.2713,  0.2579, -0.1441,  0.2039],\n",
      "        [ 0.1368, -0.2663,  0.0962, -0.3669, -0.0129, -0.2139,  0.1035, -0.0623,\n",
      "          0.1306,  0.0978, -0.1128,  0.2009, -0.0036, -0.2361,  0.0915,  0.1453]])\n",
      "\n",
      "blocks.2.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0375, -0.1867, -0.0481, -0.0833])\n",
      "\n",
      "blocks.2.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([0.8549, 1.0380, 0.9976, 0.9551])\n",
      "\n",
      "blocks.2.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0451, -0.0037, -0.0064,  0.0462])\n",
      "\n",
      "blocks.2.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.0683, 1.0866, 1.1403, 0.8972])\n",
      "\n",
      "blocks.2.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0368,  0.0260, -0.0629, -0.0617])\n",
      "\n",
      "blocks.3.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1708,  0.1578,  0.3210,  0.4083],\n",
      "        [-0.1596,  0.1862,  0.1232, -0.4277]])\n",
      "\n",
      "blocks.3.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4336, -0.3973,  0.4171, -0.4172],\n",
      "        [-0.3855,  0.1337,  0.3199,  0.3985]])\n",
      "\n",
      "blocks.3.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1098, -0.2849,  0.1161,  0.3157],\n",
      "        [ 0.1914,  0.1386,  0.1358,  0.5281]])\n",
      "\n",
      "blocks.3.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2694, -0.3461,  0.4566, -0.3278],\n",
      "        [-0.4095,  0.5132,  0.0752,  0.1731]])\n",
      "\n",
      "blocks.3.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2647, -0.0223,  0.5617, -0.3848],\n",
      "        [-0.1110,  0.3979, -0.4685,  0.5986]])\n",
      "\n",
      "blocks.3.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2139, -0.0422, -0.2590, -0.2705],\n",
      "        [-0.4982,  0.3003,  0.1155, -0.4087]])\n",
      "\n",
      "blocks.3.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.2289,  0.5775,  0.1329,  0.2258],\n",
      "        [-0.1601, -0.1950, -0.1402, -0.0781],\n",
      "        [-0.3517,  0.0987, -0.0062, -0.1326],\n",
      "        [ 0.0334, -0.1945,  0.1073,  0.0874]])\n",
      "\n",
      "blocks.3.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([-0.3023, -0.3620,  0.2893,  0.1143])\n",
      "\n",
      "blocks.3.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.3077,  0.7553, -0.4418,  0.3579],\n",
      "        [ 0.2666, -0.5473,  0.4573, -0.4157],\n",
      "        [-0.2213,  0.0710, -0.5344,  0.4489],\n",
      "        [-0.0513,  0.3749, -0.6497,  0.3242],\n",
      "        [-0.1607, -0.2951, -0.0470,  0.2863],\n",
      "        [ 0.5758, -0.4274, -0.1831,  0.0455],\n",
      "        [-0.0486,  0.1106,  0.4374, -0.0183],\n",
      "        [ 0.3562,  0.4491, -0.3264,  0.3510],\n",
      "        [ 0.1886,  0.6780, -0.3310, -0.2713],\n",
      "        [-0.2771,  0.0613,  0.3216, -0.2603],\n",
      "        [ 0.0218, -0.2003,  0.4822,  0.0554],\n",
      "        [-0.3429, -0.2420, -0.4234,  0.1984],\n",
      "        [-0.2019,  0.1716, -0.0148, -0.3467],\n",
      "        [ 0.1480, -0.4458, -0.3953, -0.4179],\n",
      "        [-0.0689, -0.2642,  0.0967, -0.1045],\n",
      "        [ 0.2589,  0.6283, -0.0284, -0.1531]])\n",
      "\n",
      "blocks.3.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.3512,  0.2998, -0.2638,  0.3572,  0.2582,  0.2458,  0.4798,  0.6473,\n",
      "         0.0979, -0.3831,  0.4778, -0.3212, -0.2611,  0.5236,  0.3919,  0.1977])\n",
      "\n",
      "blocks.3.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[ 0.2270, -0.3043,  0.0949,  0.3895,  0.0634, -0.1243, -0.1004,  0.0411,\n",
      "          0.3523, -0.0014,  0.0795,  0.0361,  0.1388,  0.0066, -0.0297,  0.2180],\n",
      "        [ 0.0987,  0.2817,  0.1291, -0.1289, -0.1971,  0.0080,  0.1063, -0.1863,\n",
      "         -0.2811,  0.1949,  0.2613, -0.0191, -0.1424, -0.1486,  0.0232, -0.1908],\n",
      "        [ 0.2978,  0.0983,  0.2619,  0.1002, -0.1066,  0.1770, -0.3060,  0.2069,\n",
      "          0.2039, -0.0838, -0.2830,  0.2505,  0.2547,  0.1937,  0.1138,  0.0653],\n",
      "        [-0.2695,  0.1583, -0.1318, -0.2422, -0.0929,  0.1232, -0.1041, -0.2072,\n",
      "         -0.0448, -0.1386,  0.0150,  0.1464,  0.0305,  0.0043,  0.3264, -0.2312]])\n",
      "\n",
      "blocks.3.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.1837, -0.1063, -0.1084, -0.0898])\n",
      "\n",
      "blocks.3.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([0.8500, 1.0033, 1.0601, 0.9525])\n",
      "\n",
      "blocks.3.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0451,  0.0452, -0.1427, -0.0989])\n",
      "\n",
      "blocks.3.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([0.9292, 1.1557, 1.2022, 0.9755])\n",
      "\n",
      "blocks.3.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1391,  0.0228, -0.0092, -0.0313])\n",
      "\n",
      "blocks.4.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.0274,  0.2154,  0.1992, -0.2863],\n",
      "        [-0.4068, -0.1383, -0.1525, -0.1254]])\n",
      "\n",
      "blocks.4.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.6243,  0.1142, -0.2199, -0.3510],\n",
      "        [-0.3501,  0.0695, -0.4439,  0.0289]])\n",
      "\n",
      "blocks.4.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3308,  0.2206,  0.4038, -0.5072],\n",
      "        [-0.3432, -0.1383,  0.4258, -0.0165]])\n",
      "\n",
      "blocks.4.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.4127, -0.0514, -0.5476,  0.0417],\n",
      "        [-0.2659,  0.5092,  0.0444, -0.0016]])\n",
      "\n",
      "blocks.4.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.0599, -0.2400,  0.4082,  0.1652],\n",
      "        [ 0.2666, -0.1340,  0.3860, -0.2120]])\n",
      "\n",
      "blocks.4.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1305,  0.3042,  0.3880,  0.4723],\n",
      "        [ 0.1110, -0.1592,  0.2662,  0.3653]])\n",
      "\n",
      "blocks.4.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.4834, -0.0955, -0.3208,  0.1105],\n",
      "        [ 0.2483,  0.0547,  0.0670,  0.2387],\n",
      "        [-0.1419, -0.4213,  0.3759, -0.2018],\n",
      "        [-0.1091,  0.0703,  0.0236, -0.4427]])\n",
      "\n",
      "blocks.4.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1525, -0.0746, -0.2372,  0.1491])\n",
      "\n",
      "blocks.4.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[-0.3673, -0.1338, -0.2118, -0.1338],\n",
      "        [-0.1848, -0.1138,  0.1415, -0.5056],\n",
      "        [-0.2548,  0.0110, -0.1667,  0.3922],\n",
      "        [-0.4476,  0.2606,  0.3061,  0.2080],\n",
      "        [-0.5830, -0.1308, -0.3277, -0.3198],\n",
      "        [ 0.1227, -0.1904,  0.3489, -0.1460],\n",
      "        [ 0.1108, -0.2482,  0.1430, -0.2820],\n",
      "        [-0.2241, -0.2998, -0.1054,  0.4732],\n",
      "        [ 0.2576, -0.4918,  0.1751,  0.0699],\n",
      "        [-0.5520,  0.4057,  0.0237,  0.1644],\n",
      "        [-0.4455,  0.0592, -0.1536,  0.3107],\n",
      "        [ 0.0920, -0.2011,  0.4741, -0.1081],\n",
      "        [-0.2700, -0.3023,  0.6537, -0.0751],\n",
      "        [ 0.0709, -0.1738, -0.1916, -0.5408],\n",
      "        [-0.5552,  0.4125,  0.3492,  0.4877],\n",
      "        [-0.1702,  0.3563, -0.4582,  0.1608]])\n",
      "\n",
      "blocks.4.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([ 0.3125,  0.4017,  0.2977,  0.2312,  0.0053,  0.2801, -0.2467,  0.1661,\n",
      "         0.0206,  0.0090, -0.4381, -0.1360, -0.1586, -0.1168, -0.3287, -0.5600])\n",
      "\n",
      "blocks.4.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[-0.0823,  0.0553,  0.2250,  0.0623, -0.1708, -0.3578,  0.0180,  0.1172,\n",
      "         -0.4002,  0.0397,  0.4851, -0.1724, -0.3374,  0.5164,  0.0898,  0.1969],\n",
      "        [-0.1557,  0.1121, -0.1647,  0.3278,  0.0037,  0.1694,  0.0653,  0.0463,\n",
      "          0.0822,  0.2104, -0.1943, -0.1470,  0.0712, -0.1652,  0.0520,  0.0535],\n",
      "        [-0.1248,  0.2582,  0.0529, -0.0145, -0.2403, -0.0796,  0.0076, -0.2081,\n",
      "          0.3465, -0.3862, -0.2907,  0.2662,  0.1157,  0.1564, -0.3964,  0.2393],\n",
      "        [ 0.0230, -0.2763,  0.3428, -0.0297, -0.0397,  0.1746, -0.1393, -0.0668,\n",
      "         -0.0044,  0.0086,  0.4090, -0.1691, -0.0036, -0.2517, -0.0293, -0.2696]])\n",
      "\n",
      "blocks.4.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0548,  0.0150, -0.1314,  0.1987])\n",
      "\n",
      "blocks.4.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([0.9322, 0.9406, 1.0655, 0.9504])\n",
      "\n",
      "blocks.4.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1190,  0.0637,  0.0587, -0.0460])\n",
      "\n",
      "blocks.4.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.0952, 1.1146, 1.2507, 1.0083])\n",
      "\n",
      "blocks.4.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1102, -0.1362,  0.0148, -0.0593])\n",
      "\n",
      "blocks.5.sa_heads.heads.0.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3956,  0.3821, -0.2003, -0.4222],\n",
      "        [ 0.6127,  0.5163, -0.3976,  0.1399]])\n",
      "\n",
      "blocks.5.sa_heads.heads.0.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.1865,  0.1932,  0.3228,  0.6607],\n",
      "        [-0.4922,  0.4637, -0.3517,  0.1027]])\n",
      "\n",
      "blocks.5.sa_heads.heads.0.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[-0.2362, -0.1329, -0.1309, -0.0103],\n",
      "        [ 0.0616, -0.0123,  0.3958,  0.3378]])\n",
      "\n",
      "blocks.5.sa_heads.heads.1.key.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.3807,  0.3728,  0.6456, -0.5083],\n",
      "        [ 0.0220,  0.2222, -0.3259, -0.7102]])\n",
      "\n",
      "blocks.5.sa_heads.heads.1.query.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4026,  0.2623, -0.2360, -0.6023],\n",
      "        [ 0.2477,  0.4926, -0.2816,  0.1907]])\n",
      "\n",
      "blocks.5.sa_heads.heads.1.value.weight\n",
      "shape = (2, 4)\n",
      "tensor([[ 0.4068, -0.3383, -0.5721, -0.3483],\n",
      "        [-0.2799, -0.2270,  0.1475, -0.1775]])\n",
      "\n",
      "blocks.5.sa_heads.proj.weight\n",
      "shape = (4, 4)\n",
      "tensor([[-0.1656, -0.1097, -0.2639,  0.5075],\n",
      "        [-0.1593, -0.3444,  0.0372,  0.2012],\n",
      "        [ 0.3693,  0.2465,  0.5536, -0.5113],\n",
      "        [ 0.1983,  0.3687, -0.0995,  0.3865]])\n",
      "\n",
      "blocks.5.sa_heads.proj.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.2968,  0.1489,  0.1139, -0.1365])\n",
      "\n",
      "blocks.5.ffwd.net.0.weight\n",
      "shape = (16, 4)\n",
      "tensor([[ 0.0752,  0.0332,  0.1467, -0.3996],\n",
      "        [ 0.3135, -0.5597,  0.5352,  0.4508],\n",
      "        [ 0.0935, -0.3021,  0.3741,  0.0368],\n",
      "        [ 0.6287,  0.1844,  0.3283, -0.4128],\n",
      "        [ 0.1101,  0.1683,  0.0235,  0.2944],\n",
      "        [-0.4690, -0.0117, -0.0141,  0.0785],\n",
      "        [ 0.4669,  0.1990, -0.4638, -0.2762],\n",
      "        [-0.2986,  0.1316, -0.0520, -0.5114],\n",
      "        [-0.1478,  0.0660, -0.3436,  0.5563],\n",
      "        [ 0.0996,  0.1716,  0.3822, -0.6252],\n",
      "        [ 0.1703,  0.3401, -0.2472, -0.4891],\n",
      "        [ 0.3608,  0.3222, -0.3083,  0.3921],\n",
      "        [-0.1037, -0.3801, -0.0347,  0.1156],\n",
      "        [-0.2631,  0.1581, -0.5365,  0.4867],\n",
      "        [ 0.5878,  0.2184,  0.1713,  0.0306],\n",
      "        [-0.2451, -0.5557,  0.2439, -0.0722]])\n",
      "\n",
      "blocks.5.ffwd.net.0.bias\n",
      "shape = (16,)\n",
      "tensor([-0.4417,  0.4082,  0.2248, -0.3760,  0.0786,  0.4040,  0.4028,  0.2352,\n",
      "         0.3998, -0.1829,  0.3076,  0.4220, -0.5634, -0.2577,  0.0336,  0.3276])\n",
      "\n",
      "blocks.5.ffwd.net.2.weight\n",
      "shape = (4, 16)\n",
      "tensor([[ 0.2625, -0.1868, -0.2373,  0.5376,  0.0745,  0.2041,  0.1114, -0.2590,\n",
      "          0.2163,  0.1927, -0.0620,  0.2972,  0.0307,  0.3237,  0.2527, -0.3565],\n",
      "        [-0.1511,  0.1774, -0.0566, -0.2800, -0.0880,  0.2903, -0.0893,  0.0408,\n",
      "         -0.1658, -0.3193,  0.0629,  0.1574, -0.2086,  0.0750, -0.2629,  0.2612],\n",
      "        [ 0.0763,  0.2132, -0.1512,  0.2539, -0.0092, -0.1553,  0.2757, -0.0722,\n",
      "         -0.2094, -0.2350,  0.1120,  0.0584, -0.0931, -0.2916, -0.0159,  0.1656],\n",
      "        [-0.1373, -0.0473,  0.0246, -0.2488,  0.0750, -0.0082,  0.2325,  0.0145,\n",
      "         -0.1161,  0.3331,  0.0521, -0.0748, -0.0649,  0.0007, -0.3230,  0.0291]])\n",
      "\n",
      "blocks.5.ffwd.net.2.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.1158, -0.1251, -0.2087, -0.1429])\n",
      "\n",
      "blocks.5.ln1.weight\n",
      "shape = (4,)\n",
      "tensor([1.0615, 1.1100, 1.0646, 1.1616])\n",
      "\n",
      "blocks.5.ln1.bias\n",
      "shape = (4,)\n",
      "tensor([ 0.0131,  0.0884, -0.0458,  0.0096])\n",
      "\n",
      "blocks.5.ln2.weight\n",
      "shape = (4,)\n",
      "tensor([1.1682, 1.0056, 1.1198, 1.1355])\n",
      "\n",
      "blocks.5.ln2.bias\n",
      "shape = (4,)\n",
      "tensor([-0.0617,  0.0044, -0.0567,  0.0637])\n",
      "\n",
      "ln_f.weight\n",
      "shape = (4,)\n",
      "tensor([1.6798, 1.5809, 1.5496, 1.7819])\n",
      "\n",
      "ln_f.bias\n",
      "shape = (4,)\n",
      "tensor([-0.1296, -0.0208,  0.1399,  0.1981])\n",
      "\n",
      "lm_head.weight\n",
      "shape = (5, 4)\n",
      "tensor([[ 1.0071,  0.4824, -0.7760,  0.7885],\n",
      "        [-0.5552, -0.8505,  0.9916,  0.1063],\n",
      "        [ 0.2769, -0.7281,  0.2721,  1.0662],\n",
      "        [ 0.8958, -0.4566,  0.4908, -0.9547],\n",
      "        [-0.0476,  0.7546, -0.3116, -0.3612]])\n",
      "\n",
      "lm_head.bias\n",
      "shape = (5,)\n",
      "tensor([ 0.0203, -0.3107, -0.0794, -0.0462,  0.1639])\n"
     ]
    }
   ],
   "source": [
    "final_params = collect_params(model)\n",
    "logits_final = logits.detach().cpu().clone()\n",
    "print_params(final_params, \"AFTER LAST OPTIMIZER STEP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6be132eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5136,  0.4143,  0.4980,  1.4906,  0.2215],\n",
      "        [-0.0714, -0.6300,  0.2959,  1.2522,  1.1193],\n",
      "        [-0.1393,  0.5444,  0.3458,  0.4671,  0.5524],\n",
      "        [ 0.2334, -0.8939,  0.1138,  0.6010,  1.4805],\n",
      "        [-0.6890,  0.4714,  0.3167,  1.2914, -0.3050],\n",
      "        [-0.4228,  0.5005,  0.5003,  1.2855,  0.3269],\n",
      "        [-0.2089, -0.1537,  0.4843,  1.4844,  0.9307],\n",
      "        [-0.0242, -0.6088,  0.3070,  1.1882,  1.1971]])\n",
      "tensor([[-0.5136,  0.4143,  0.4980,  1.4906,  0.2215],\n",
      "        [-0.0714, -0.6300,  0.2959,  1.2522,  1.1193],\n",
      "        [-0.1393,  0.5444,  0.3458,  0.4671,  0.5524],\n",
      "        [ 0.2334, -0.8939,  0.1138,  0.6010,  1.4805],\n",
      "        [-0.6890,  0.4714,  0.3167,  1.2914, -0.3050],\n",
      "        [-0.4228,  0.5005,  0.5003,  1.2855,  0.3269],\n",
      "        [-0.2089, -0.1537,  0.4843,  1.4844,  0.9307],\n",
      "        [-0.0242, -0.6088,  0.3070,  1.1882,  1.1971],\n",
      "        [-0.4545,  0.3631,  0.5244,  1.5056,  0.3694],\n",
      "        [ 0.3703, -1.0020, -0.0116,  0.1999,  1.5831],\n",
      "        [-0.3133,  0.6358,  0.2436,  0.4176,  0.1125],\n",
      "        [-0.1331, -0.6248,  0.2932,  1.3346,  1.0178],\n",
      "        [-0.5597,  0.1979,  0.4736,  1.6887,  0.2158],\n",
      "        [-0.2852,  0.2051,  0.5473,  1.4376,  0.7316],\n",
      "        [ 0.5216, -1.2282, -0.6841, -1.3743,  1.0706],\n",
      "        [-0.3494, -0.2400,  0.4305,  1.6465,  0.6834],\n",
      "        [-0.5921,  0.1793,  0.4506,  1.7007,  0.1448],\n",
      "        [ 0.3441, -0.8605,  0.0825,  0.3341,  1.5981],\n",
      "        [-0.3037,  0.6315,  0.3252,  0.5936,  0.2324],\n",
      "        [ 0.0277, -0.6261,  0.2953,  1.0981,  1.2706],\n",
      "        [-0.5517,  0.2601,  0.4851,  1.6499,  0.2149],\n",
      "        [-0.5089,  0.1750,  0.4957,  1.6822,  0.3328],\n",
      "        [ 0.5895, -0.7775, -0.4030, -1.2177,  1.3293],\n",
      "        [-0.1832, -0.7695,  0.1960,  1.3108,  0.8820],\n",
      "        [-0.4693, -0.0702,  0.4504,  1.7278,  0.4466],\n",
      "        [ 0.2217, -0.9406,  0.0898,  0.6035,  1.4528],\n",
      "        [ 0.2698,  0.0041, -0.2379, -1.0244,  0.6925],\n",
      "        [-0.0839, -0.7790,  0.2099,  1.2012,  1.0579],\n",
      "        [-0.5595,  0.4895,  0.4541,  1.3951,  0.0613],\n",
      "        [-0.5252, -0.0186,  0.4401,  1.7447,  0.3238],\n",
      "        [-0.0662,  0.1741,  0.5081,  1.0443,  1.0360],\n",
      "        [ 0.2561, -1.0551,  0.0050,  0.4554,  1.4506]])\n",
      "tensor([[-0.6143,  2.9987,  3.9816, -2.7295, -2.3400],\n",
      "        [-2.1338,  1.1315, -1.0060,  4.5579, -0.2739],\n",
      "        [-0.5858, -2.2842, -3.5800, -1.6534,  2.7209],\n",
      "        [ 4.3970, -3.1505,  1.4128, -1.9046,  0.2528],\n",
      "        [-4.2595,  3.6689,  0.2863, -0.9379, -0.7450],\n",
      "        [-0.3053,  2.5669,  3.7923, -3.4254, -2.0473],\n",
      "        [-1.0070, -0.2533, -1.5812,  4.6530,  0.3210],\n",
      "        [-0.4542, -2.4425, -3.6511, -1.4923,  2.7707],\n",
      "        [ 4.1580, -4.0068, -0.0906, -2.1725,  1.2849],\n",
      "        [-4.0802,  4.0920,  1.1020, -0.7477, -1.3023],\n",
      "        [ 3.0593, -0.3032,  3.7947, -1.7301, -1.6684],\n",
      "        [-1.1905,  0.6356, -0.5453,  4.7004, -0.4353],\n",
      "        [-1.5077, -0.8262, -2.5753, -2.9240,  2.0440],\n",
      "        [ 4.4088, -3.0808,  1.4908, -1.2955,  0.1103],\n",
      "        [-2.7803,  4.0695,  2.7547, -1.8328, -2.0356],\n",
      "        [ 0.9852,  1.8748,  4.3602, -2.5086, -2.3329],\n",
      "        [-0.0070,  2.5970,  4.1683, -2.7688, -2.3486],\n",
      "        [-1.1470,  0.1239, -1.2214,  4.7092,  0.0379],\n",
      "        [-0.8307, -2.0622, -3.5691, -1.6486,  2.6665],\n",
      "        [ 4.4029, -3.0622,  1.5043, -1.1981,  0.0853],\n",
      "        [-4.2106,  4.1077,  0.9365, -0.3220, -1.2764],\n",
      "        [ 0.4382,  1.8883,  3.7748, -3.8191, -1.8364],\n",
      "        [ 1.0116, -1.7375, -1.1507,  4.1441,  0.4802],\n",
      "        [-0.8166, -2.0265, -3.4909, -1.8117,  2.6396],\n",
      "        [ 3.3291, -3.9396, -0.9764, -3.0506,  1.8710],\n",
      "        [-3.7393,  3.8686,  1.2656, -1.7165, -1.2064],\n",
      "        [ 1.6307,  1.2116,  4.2371, -2.8089, -2.0815],\n",
      "        [-0.5997, -0.8119, -1.8631,  4.5355,  0.6097],\n",
      "        [-0.2655, -2.4080, -3.3272, -2.1594,  2.6824],\n",
      "        [ 4.0022, -4.3688, -0.8722, -0.9701,  1.6151],\n",
      "        [-1.1067,  3.1087,  3.5490, -3.1456, -2.0722],\n",
      "        [ 0.8217,  1.8177,  4.1198, -3.2674, -2.0845]])\n"
     ]
    }
   ],
   "source": [
    "print(logits_before_opt[0:8])\n",
    "print(logits_initial)\n",
    "print(logits_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5d9ae2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[4, 0, 1, 2, 3, 4, 0, 1],\n",
      "        [1, 2, 3, 4, 0, 1, 2, 3],\n",
      "        [1, 2, 3, 4, 0, 1, 2, 3],\n",
      "        [0, 1, 2, 3, 4, 0, 1, 2]], device='cuda:0'), tensor([[0, 1, 2, 3, 4, 0, 1, 2],\n",
      "        [2, 3, 4, 0, 1, 2, 3, 4],\n",
      "        [2, 3, 4, 0, 1, 2, 3, 4],\n",
      "        [1, 2, 3, 4, 0, 1, 2, 3]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcda8c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " d abcd abcd d abcd  abcabcd abcd abcd abcd abcd abcd abcd abcd abcd d acd abcd abbcd acd abcd ad abc\n"
     ]
    }
   ],
   "source": [
    "context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "print(decode(model.generate(context,max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419ee75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cc30ede",
   "metadata": {},
   "source": [
    "## Self Attention test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d002bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C=4,8,2 #batch, time, channels\n",
    "x=torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9eff967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 1\n",
    "xbow=torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev=x[b,:t+1]\n",
    "        xbow[b,t]=torch.mean(xprev,0)\n",
    "        \n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dc2abf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei=wei/wei.sum(1, keepdim=True)\n",
    "wei\n",
    "xbow2=wei @ x # wei-> B * T * T  x -> B * T * C, output B * T * C\n",
    "xbow2[0]\n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1751577d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei=torch.zeros((T,T))\n",
    "wei=wei.masked_fill(tril==0,float('-inf'))\n",
    "wei=F.softmax(wei,dim=-1)\n",
    "\n",
    "xbow3=wei @ x\n",
    "torch.allclose(xbow2,xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04504e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C=4,8,32 #batch, time, channels\n",
    "x=torch.randn(B,T,C)\n",
    "\n",
    "# single head\n",
    "head_size=16\n",
    "key=nn.Linear(C,head_size,bias=False)\n",
    "query=nn.Linear(C,head_size,bias=False)\n",
    "value=nn.Linear(C,head_size,bias=False)\n",
    "\n",
    "k=key(x) # (B,T,16 head_size)\n",
    "q=query(x) # (B,T,16 head_size)\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,16) * (B,16,T) --> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei=torch.zeros((T,T))\n",
    "wei=wei.masked_fill(tril==0,float('-inf'))\n",
    "wei=F.softmax(wei,dim=-1)\n",
    "\n",
    "v=value(x)\n",
    "out=wei @ v\n",
    "\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19e1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ea946eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 7.0000],\n",
       "        [4.0000, 5.5000],\n",
       "        [4.6667, 5.3333]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a=a/torch.sum(a,1, keepdim=True)\n",
    "b=torch.randint(0,10,(3,2)).float()\n",
    "c=a@b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ed8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e8088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f56ea367",
   "metadata": {},
   "source": [
    "## Old test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=8 # context length\n",
    "batch_size=4 # independent sequences in parallel\n",
    "ix=torch.randint(len(data)-block_size,(batch_size,))\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e67a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabfea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "tensor([[0.6789, 0.4387],\n",
      "        [0.5568, 0.1342]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.rand(2,2).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f392e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
