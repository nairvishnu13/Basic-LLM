{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c8494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's start\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab0515c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "with open ('input_abcd.txt', 'r', encoding='utf-8') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd753d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  244720\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be63d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcd abcd \n"
     ]
    }
   ],
   "source": [
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c1d8156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " abcd\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# unique characters\n",
    "chars=sorted(list(set(text)))\n",
    "vocab_size=len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46abb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create character to integer mapping\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # input string and output integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # input integers and outpu string\n",
    "\n",
    "#print(encode(\"hii there\"))\n",
    "#print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6359fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question for future 1: large vocab/small sequence how is the data read to split into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05906064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([244720]) torch.int64\n",
      "tensor([1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3,\n",
      "        4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\n",
      "        3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1,\n",
      "        2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\n",
      "        1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4,\n",
      "        0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0])\n"
     ]
    }
   ],
   "source": [
    "#encode entire text and store into torch tensor\n",
    "import torch\n",
    "data=torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # first 100 characters - encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2060d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation set\n",
    "n=int(0.9*len(data))\n",
    "train_data=data[:n]\n",
    "val_data=data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91179280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size=8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e1daac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([1]) the target: 2\n",
      "when input is tensor([1, 2]) the target: 3\n",
      "when input is tensor([1, 2, 3]) the target: 4\n",
      "when input is tensor([1, 2, 3, 4]) the target: 0\n",
      "when input is tensor([1, 2, 3, 4, 0]) the target: 1\n",
      "when input is tensor([1, 2, 3, 4, 0, 1]) the target: 2\n",
      "when input is tensor([1, 2, 3, 4, 0, 1, 2]) the target: 3\n",
      "when input is tensor([1, 2, 3, 4, 0, 1, 2, 3]) the target: 4\n"
     ]
    }
   ],
   "source": [
    "x=train_data[:block_size]\n",
    "y=train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4a97ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([64, 128])\n",
      "tensor([[1, 2, 3,  ..., 1, 2, 3],\n",
      "        [3, 4, 0,  ..., 3, 4, 0],\n",
      "        [3, 4, 0,  ..., 3, 4, 0],\n",
      "        ...,\n",
      "        [2, 3, 4,  ..., 2, 3, 4],\n",
      "        [3, 4, 0,  ..., 3, 4, 0],\n",
      "        [2, 3, 4,  ..., 2, 3, 4]], device='cuda:0')\n",
      "targets:\n",
      "torch.Size([64, 128])\n",
      "tensor([[2, 3, 4,  ..., 2, 3, 4],\n",
      "        [4, 0, 1,  ..., 4, 0, 1],\n",
      "        [4, 0, 1,  ..., 4, 0, 1],\n",
      "        ...,\n",
      "        [3, 4, 0,  ..., 3, 4, 0],\n",
      "        [4, 0, 1,  ..., 4, 0, 1],\n",
      "        [3, 4, 0,  ..., 3, 4, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "#hyperparameters\n",
    "batch_size=64 # independent sequences in parallel\n",
    "block_size=128 # context length\n",
    "max_iters=5000\n",
    "eval_interval=500\n",
    "learning_rate=3e-4\n",
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters=200\n",
    "n_embd=192\n",
    "n_head=6\n",
    "n_layer=6\n",
    "dropout=0.1\n",
    "\n",
    "#--------------------\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate a small batch of inputs x and targets y\n",
    "    data=train_data if split=='train' else val_data\n",
    "    ix=torch.randint(len(data)-block_size,(batch_size,))\n",
    "    #print(ix)\n",
    "    x=torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y=x.to(device),y.to(device)\n",
    "    return x,y\n",
    "\n",
    "xb, yb=get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "# for i in range(batch_size):  #batch dimension - rows\n",
    "#     for t in range(block_size): #time dimension - column\n",
    "#         context=xb[i,:t+1]\n",
    "#         target=yb[i,t]\n",
    "#         print(f\"when input is {context} output is: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62241e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','val']:\n",
    "        losses=torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y=get_batch(split)\n",
    "            X,Y=X.to(device),Y.to(device)\n",
    "            logits,loss=model(X,Y)\n",
    "            losses[k]=loss.item()\n",
    "        out[split]=losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0f5c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key=nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.query=nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.value=nn.Linear(n_embd,head_size,bias=False)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B,T,C = x.shape\n",
    "        k=self.key(x) # (B,T,C head_size)\n",
    "        q=self.query(x) # (B,T,C head_size)\n",
    "        # compute attention scores - affinities\n",
    "        wei = q @ k.transpose(-2,-1) *C**-0.5 # (B,T,16) * (B,16,T) --> (B,T,T)\n",
    "        wei=wei.masked_fill(self.tril[:T,:T]==0,float('-inf')) #(B,T,T)\n",
    "        wei=F.softmax(wei,dim=-1) #(B,T,T)\n",
    "        wei=self.dropout(wei)\n",
    "        # perform weighted aggregation of the values\n",
    "        v=self.value(x) # (B,T,C)\n",
    "        out=wei @ v #(B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd,n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out=torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "715dafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd,4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd,n_embd), #projection layer, added in multi head attention\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd,n_head):\n",
    "        #n_embd - number of embeddings, n_head = number of heads\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa_heads=MultiHeadAttention(n_head,head_size) # heads of dimensional self-attention\n",
    "        self.ffwd=FeedForward(n_embd)\n",
    "        self.ln1=nn.LayerNorm(n_embd)\n",
    "        self.ln2=nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x + self.sa_heads(self.ln1(x)) # apply 4 heads of 8 dimensional self attention (B,T,C)\n",
    "        x=x + self.ffwd(self.ln2(x)) # (B, T, C)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4faa6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "\n",
    "# class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         #each token directly reads off the logits for the next token from a lookup table\n",
    "#         self.token_embedding_table=nn.Embedding(vocab_size,n_embd)\n",
    "#         self.position_embedding_table=nn.Embedding(block_size,n_embd)\n",
    "#         #self.sa_head=Head(n_embd)\n",
    "#         self.sa_heads=MultiHeadAttention(4,n_embd//4) # 4 heads of 8 dimensional self-attention\n",
    "#         self.ffwd=FeedForward(n_embd)\n",
    "#         self.lm_head=nn.Linear(n_embd,vocab_size)\n",
    "    \n",
    "#     def forward(self,idx,targets=None):\n",
    "#         B,T = idx.shape\n",
    "#         #idx and targets are both B,T tensor of integers\n",
    "#         tok_emb= self.token_embedding_table(idx) #(B,T,C)\n",
    "#         pos_emb=self.position_embedding_table(torch.arange(T,device=device)) #(T,C)\n",
    "#         x = tok_emb + pos_emb #(B,T,C)\n",
    "#         #x=self.sa_head(x) # apply one head of self attention (B,T,C)\n",
    "#         x=self.sa_heads(x) # apply 4 heads of 8 dimensional self attention (B,T,C)\n",
    "#         x=self.ffwd(x) # (B, T, C)\n",
    "#         logits=self.lm_head(x) #(B,T,vocab_size)   \n",
    "\n",
    "#         if targets is None:\n",
    "#             loss = None\n",
    "#         else: \n",
    "#             B,T,C=logits.shape\n",
    "#             logits=logits.view(B*T,C)\n",
    "#             targets=targets.view(B*T)\n",
    "#             loss=F.cross_entropy(logits,targets)\n",
    "\n",
    "#         return logits,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70b2f02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 5])\n",
      "tensor(1.7590, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size,n_embd)\n",
    "        self.position_embedding_table=nn.Embedding(block_size,n_embd)\n",
    "        # self.blocks = nn.Sequential(\n",
    "        #     Block(n_embd,n_head=4),\n",
    "        #     Block(n_embd,n_head=4),\n",
    "        #     Block(n_embd,n_head=4),\n",
    "        #     nn.LayerNorm(n_embd)\n",
    "        # )\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f=nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head=nn.Linear(n_embd,vocab_size)\n",
    "    \n",
    "    def forward(self,idx,targets=None):\n",
    "        B,T = idx.shape\n",
    "        #idx and targets are both B,T tensor of integers\n",
    "        tok_emb= self.token_embedding_table(idx) #(B,T,C)\n",
    "        pos_emb=self.position_embedding_table(torch.arange(T,device=device)) #(T,C)\n",
    "        x = tok_emb + pos_emb #(B,T,C)\n",
    "        #x=self.sa_head(x) # apply one head of self attention (B,T,C)\n",
    "        x=self.blocks(x) #(B,T,C)\n",
    "        x=self.ln_f(x) #(B,T,C)\n",
    "        logits=self.lm_head(x) #(B,T,vocab_size)   \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            B,T,C=logits.shape\n",
    "            logits=logits.view(B*T,C)\n",
    "            targets=targets.view(B*T)\n",
    "            loss=F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #crop idx to last block_size tokens\n",
    "            idx_cond=idx[:,-block_size:]\n",
    "            # get the predictions\n",
    "            logits,loss =self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits=logits[:,-1,:] #becomes B,C\n",
    "            # apply softmax to get the probabilities\n",
    "            probs=F.softmax(logits,dim=-1) # B,C\n",
    "            # sample from the distribution\n",
    "            idx_next=torch.multinomial(probs,num_samples=1) # B,1\n",
    "            # append sample index to the running sequence\n",
    "            idx=torch.cat((idx, idx_next),dim=1) #(B,T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model=BigramLanguageModel()\n",
    "model=model.to(device)\n",
    "#out=m(xb,yb)\n",
    "logits,loss=model(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx=torch.zeros((1,1),dtype=torch.long)\n",
    "#print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "981a61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer=torch.optim.AdamW(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ea01ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.7644, val loss1.7645\n",
      "step 500: train loss 0.0001, val loss0.0001\n",
      "step 1000: train loss 0.0000, val loss0.0000\n",
      "step 1500: train loss 0.0000, val loss0.0000\n",
      "step 2000: train loss 0.0000, val loss0.0000\n",
      "step 2500: train loss 0.0000, val loss0.0000\n",
      "step 3000: train loss 0.0000, val loss0.0000\n",
      "step 3500: train loss 0.0000, val loss0.0000\n",
      "step 4000: train loss 0.0000, val loss0.0000\n",
      "step 4500: train loss 0.0000, val loss0.0000\n",
      "1.4253890867621521e-06\n"
     ]
    }
   ],
   "source": [
    "#batch_size=32\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # evey once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval==0:\n",
    "        losses=estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss{losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    (xb,yb)=get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits,loss=model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcda8c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd abcd \n"
     ]
    }
   ],
   "source": [
    "context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
    "print(decode(model.generate(context,max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419ee75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cc30ede",
   "metadata": {},
   "source": [
    "## Self Attention test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d002bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C=4,8,2 #batch, time, channels\n",
    "x=torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9eff967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 1\n",
    "xbow=torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev=x[b,:t+1]\n",
    "        xbow[b,t]=torch.mean(xprev,0)\n",
    "        \n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dc2abf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei=wei/wei.sum(1, keepdim=True)\n",
    "wei\n",
    "xbow2=wei @ x # wei-> B * T * T  x -> B * T * C, output B * T * C\n",
    "xbow2[0]\n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1751577d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei=torch.zeros((T,T))\n",
    "wei=wei.masked_fill(tril==0,float('-inf'))\n",
    "wei=F.softmax(wei,dim=-1)\n",
    "\n",
    "xbow3=wei @ x\n",
    "torch.allclose(xbow2,xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04504e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C=4,8,32 #batch, time, channels\n",
    "x=torch.randn(B,T,C)\n",
    "\n",
    "# single head\n",
    "head_size=16\n",
    "key=nn.Linear(C,head_size,bias=False)\n",
    "query=nn.Linear(C,head_size,bias=False)\n",
    "value=nn.Linear(C,head_size,bias=False)\n",
    "\n",
    "k=key(x) # (B,T,16 head_size)\n",
    "q=query(x) # (B,T,16 head_size)\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,16) * (B,16,T) --> (B,T,T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "#wei=torch.zeros((T,T))\n",
    "wei=wei.masked_fill(tril==0,float('-inf'))\n",
    "wei=F.softmax(wei,dim=-1)\n",
    "\n",
    "v=value(x)\n",
    "out=wei @ v\n",
    "\n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19e1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ea946eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 7.0000],\n",
       "        [4.0000, 5.5000],\n",
       "        [4.6667, 5.3333]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a=a/torch.sum(a,1, keepdim=True)\n",
    "b=torch.randint(0,10,(3,2)).float()\n",
    "c=a@b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ed8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e8088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f56ea367",
   "metadata": {},
   "source": [
    "## Old test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c8800",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size=8 # context length\n",
    "batch_size=4 # independent sequences in parallel\n",
    "ix=torch.randint(len(data)-block_size,(batch_size,))\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e67a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabfea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "tensor([[0.6789, 0.4387],\n",
      "        [0.5568, 0.1342]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.rand(2,2).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f392e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
